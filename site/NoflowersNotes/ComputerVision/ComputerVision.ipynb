{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Stanford - Spring 2025\n",
    "\n",
    "### Ways to compare images\n",
    "\n",
    "- L1 distances: $d_1(I_1, I_2) = \\sum_p \\vert I_1^p - I_2^p \\vert$\n",
    "- L2 distances: $d_2(I_1, I_2) = \\sqrt{\\sum_p ( I_1^p - I_2^p )^2}$\n",
    "\n",
    "A classifier below with $O(1)$ for training and $O(N)$ for predecting with L1 distance.\n"
   ],
   "id": "bbc89b587f140c62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "class NearestNeighbor(object):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def train(self, X, y):\n",
    "    \"\"\" X is N x D where each row is an example. Y is 1-dimension of size N \"\"\"\n",
    "    # the nearest neighbor classifier simply remembers all the training data\n",
    "    self.Xtr = X\n",
    "    self.ytr = y\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\" X is N x D where each row is an example we wish to predict label for \"\"\"\n",
    "    num_test = X.shape[0]\n",
    "    # let's make sure that the output type matches the input type\n",
    "    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)\n",
    "\n",
    "    # loop over all test rows\n",
    "    for i in range(num_test):\n",
    "      # find the nearest training image to the i'th test image\n",
    "      # using the L1 distance (sum of absolute value differences)\n",
    "      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)\n",
    "      min_index = np.argmin(distances) # get the index with the smallest distance\n",
    "      Ypred[i] = self.ytr[min_index] # predict the label of the nearest example\n",
    "\n",
    "    return Ypred"
   ],
   "id": "d8f49a655267c3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It's bad: we want classifiers that are fast at prediction and slow for training\n",
    "\n",
    "### K-Neatest Neighbours\n",
    "The idea is very simple: instead of finding the single closest image in the training set, we will find the top k closest images, and have them vote on the label of the test image.\n",
    "Higher values of k have a smoothing effect that makes the classifier more resistant to outliers\n",
    "\n",
    "![K-NN.png](./resources/NNClassifier.png)\n",
    "\n",
    "Ideas on the hyperparameters\n",
    "- ❌ Choose hyperparameters working best on the data. K = 1 always works best on the training data.\n",
    "- ❌ Choose hyperparameters working best on the test data. No idea how it will perform on new data.\n",
    "\n",
    "> Whenever you’re designing Machine Learning algorithms, you should think of the test set as a very precious resource that should ideally never be touched until one time at the very end.\n",
    "\n",
    "- Split data into **train**, **validation**, **test** sets, train and test on train and validation sets and showcase the performance on the test set. (Test at the last time.)"
   ],
   "id": "2fc392d173d4425b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before\n",
    "# recall Xtr_rows is 50,000 x 3072 matrix\n",
    "Xval_rows = Xtr_rows[:1000, :] # take first 1000 for validation\n",
    "Yval = Ytr[:1000]\n",
    "Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for train\n",
    "Ytr = Ytr[1000:]\n",
    "\n",
    "# find hyperparameters that work best on the validation set\n",
    "validation_accuracies = []\n",
    "for k in [1, 3, 5, 10, 20, 50, 100]:\n",
    "\n",
    "  # use a particular value of k and evaluation on validation data\n",
    "  nn = NearestNeighbor()\n",
    "  nn.train(Xtr_rows, Ytr)\n",
    "  # here we assume a modified NearestNeighbor class that can take a k as input\n",
    "  Yval_predict = nn.predict(Xval_rows, k = k)\n",
    "  acc = np.mean(Yval_predict == Yval)\n",
    "  print 'accuracy: %f' % (acc,)\n",
    "\n",
    "  # keep track of what works on the validation set\n",
    "  validation_accuracies.append((k, acc))"
   ],
   "id": "d55b3335542c34e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Cross-Validation** (use less on CV). Split data into folds, try each fold as validation and average the results.\n",
    "\n",
    "> For example, in 5-fold cross-validation, we would split the training data into 5 equal folds, use 4 of them for training, and 1 for validation. We would then iterate over which fold is the validation fold, evaluate the performance, and finally average the performance across the different folds.\n",
    "\n",
    "!!! remarks \"Tip\"\n",
    "    In practice, people prefer to avoid cross-validation in favor of having a single validation split, since cross-validation can be computationally expensive.\n",
    "\n",
    "K-Nearest Neighbor on images never used: very slow at test time, and distance matrix on pixels are not informative. And our classifier should densely cover the space, which is harder when the dimension goes high. (Curse of dimensionality)"
   ],
   "id": "4988d9753cf2bc4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Parametric Approach\n",
    "\n",
    "$$\n",
    "input: x \\rightarrow f(x, W[, b]) \\rightarrow \\text{10 numbers giving class scores}\n",
    "$$\n",
    "\n",
    "e.g.\n",
    "\n",
    "$$\n",
    "f(x, W, b) = W \\text{vec}(x) + b\n",
    "$$\n",
    "\n",
    "- Hard cases: Cases with no linear boundaries.\n",
    "\n",
    "### Loss function\n",
    "\n",
    "Given a dataset of examples $\\{(x_i, y_i)\\}_{i = 1}^n$, loss over the dataset is a sum of loss over examples:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N}\\sum_{i}L_i(f(x_i, W), y_i)\n",
    "$$\n",
    "\n",
    "#### Multiclass SVM Loss (Hinge loss)\n",
    "\n",
    "$y_i$ are integers, $s_i$ is the $j$-th class prediction of data $x_i$, $L_i = \\sum_{j \\neq y_i}\\max \\{0, s_j - s_{y_i} + 1\\}$  (s -> score).\n",
    "$1$ here is a threshold, but it can be chosen randomly.\n",
    "\n",
    "![image.png](resources/img.png)"
   ],
   "id": "9dafda8a6ce57d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def L_i_vectorized(x, y, W):\n",
    "    scores = W.dot(x)\n",
    "    margins = np.maximum(0, scores - scores[y] + 1)\n",
    "    margins[y] = 0\n",
    "    loss_i = np.sum(margins)\n",
    "    return loss_i"
   ],
   "id": "aece15bce628cdb9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### The regularization\n",
    "\n",
    "Model should be simple, so it works on test data. (防止过拟合)\n",
    "\n",
    "$$\n",
    "L(W) = \\frac{1}{N}\\sum_{i = 1}^{N}L_i(f(x_i, W), y_i) + \\lambda R(W)\n",
    "$$\n",
    "\n",
    "$\\lambda$: regularization strength.\n",
    "$R(W)$ allows the model to choose a simple model (It can be understood as reducing the degree of the fitted curve)\n",
    "\n",
    "- L2 regularization: $R(W) = \\Vert W \\Vert_F$\n",
    "- L1 regularization: $R(W) = \\Vert W \\Vert_1$\n",
    "- Elastic net: $R(W) = \\beta \\Vert W \\Vert_F + \\Vert W \\Vert_1$\n",
    "- Max norm regularization\n",
    "- Dropout\n",
    "- Fancier\n",
    "\n",
    "#### Softmax Loss (Cross-entropy loss)\n",
    "\n",
    "$$\n",
    "P(Y = k | X = x_i) = \\frac{\\mathrm{e}^{s_k}}{\\sum_j \\mathrm{e}^{s_j}}\n",
    "$$\n",
    "\n",
    "where $s = f(x_i, W)$, which will normalize the score vector $s$.\n",
    "We want to minimize the negative log likelihood of the correct class, so the loss function would be\n",
    "\n",
    "$$\n",
    "L_i = -\\log P(Y = y_i | X = x_i)\n",
    "$$\n"
   ],
   "id": "ecb452ac53549cd9"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
