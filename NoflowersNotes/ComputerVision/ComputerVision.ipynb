{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# CS231n: Deep Learning for Computer Vision\n",
    "## Stanford - Spring 2025\n",
    "\n",
    "### Image Classification\n",
    "\n",
    "#### Ways to compare images\n",
    "\n",
    "- L1 distances: $d_1(I_1, I_2) = \\sum_p \\vert I_1^p - I_2^p \\vert$\n",
    "- L2 distances: $d_2(I_1, I_2) = \\sqrt{\\sum_p ( I_1^p - I_2^p )^2}$\n",
    "\n",
    "A classifier below with $O(1)$ for training and $O(N)$ for predecting with L1 distance.\n"
   ],
   "id": "7d15a356b31c92a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "class NearestNeighbor(object):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def train(self, X, y):\n",
    "    \"\"\" X is N x D where each row is an example. Y is 1-dimension of size N \"\"\"\n",
    "    # the nearest neighbor classifier simply remembers all the training data\n",
    "    self.Xtr = X\n",
    "    self.ytr = y\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\" X is N x D where each row is an example we wish to predict label for \"\"\"\n",
    "    num_test = X.shape[0]\n",
    "    # let's make sure that the output type matches the input type\n",
    "    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)\n",
    "\n",
    "    # loop over all test rows\n",
    "    for i in range(num_test):\n",
    "      # find the nearest training image to the i'th test image\n",
    "      # using the L1 distance (sum of absolute value differences)\n",
    "      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)\n",
    "      min_index = np.argmin(distances) # get the index with the smallest distance\n",
    "      Ypred[i] = self.ytr[min_index] # predict the label of the nearest example\n",
    "\n",
    "    return Ypred"
   ],
   "id": "21f6f4a8cf67251c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It's bad: we want classifiers that are fast at prediction and slow for training\n",
    "\n",
    "#### K-Neatest Neighbours\n",
    "The idea is very simple: instead of finding the single closest image in the training set, we will find the top k closest images, and have them vote on the label of the test image.\n",
    "Higher values of k have a smoothing effect that makes the classifier more resistant to outliers\n",
    "\n",
    "![K-NN.png](./resources/NNClassifier.png)\n",
    "\n",
    "Ideas on the hyperparameters\n",
    "- ❌ Choose hyperparameters working best on the data. K = 1 always works best on the training data.\n",
    "- ❌ Choose hyperparameters working best on the test data. No idea how it will perform on new data.\n",
    "\n",
    "> Whenever you’re designing Machine Learning algorithms, you should think of the test set as a very precious resource that should ideally never be touched until one time at the very end.\n",
    "\n",
    "- Split data into **train**, **validation**, **test** sets, train and test on train and validation sets and showcase the performance on the test set. (Test at the last time.)"
   ],
   "id": "7a1e4442c2c96bc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before\n",
    "# recall Xtr_rows is 50,000 x 3072 matrix\n",
    "Xval_rows = Xtr_rows[:1000, :] # take first 1000 for validation\n",
    "Yval = Ytr[:1000]\n",
    "Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for train\n",
    "Ytr = Ytr[1000:]\n",
    "\n",
    "# find hyperparameters that work best on the validation set\n",
    "validation_accuracies = []\n",
    "for k in [1, 3, 5, 10, 20, 50, 100]:\n",
    "\n",
    "  # use a particular value of k and evaluation on validation data\n",
    "  nn = NearestNeighbor()\n",
    "  nn.train(Xtr_rows, Ytr)\n",
    "  # here we assume a modified NearestNeighbor class that can take a k as input\n",
    "  Yval_predict = nn.predict(Xval_rows, k = k)\n",
    "  acc = np.mean(Yval_predict == Yval)\n",
    "  print 'accuracy: %f' % (acc,)\n",
    "\n",
    "  # keep track of what works on the validation set\n",
    "  validation_accuracies.append((k, acc))"
   ],
   "id": "c35f52e28ee3684"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Cross-Validation** (use less on CV). Split data into folds, try each fold as validation and average the results.\n",
    "\n",
    "> For example, in 5-fold cross-validation, we would split the training data into 5 equal folds, use 4 of them for training, and 1 for validation. We would then iterate over which fold is the validation fold, evaluate the performance, and finally average the performance across the different folds.\n",
    "\n",
    "!!! tip\n",
    "    In practice, people prefer to avoid cross-validation in favor of having a single validation split, since cross-validation can be computationally expensive.\n",
    "\n",
    "K-Nearest Neighbor on images never used: very slow at test time, and distance matrix on pixels are not informative. And our classifier should densely cover the space, which is harder when the dimension goes high. (Curse of dimensionality)\n",
    "\n",
    "#### Parametric Approach\n",
    "\n",
    "$$\n",
    "input: x \\rightarrow f(x, W[, b]) \\rightarrow \\text{10 numbers giving class scores}\n",
    "$$\n",
    "\n",
    "e.g.\n",
    "\n",
    "$$\n",
    "f(x, W, b) = W \\text{vec}(x) + b\n",
    "$$\n",
    "\n",
    "- Hard cases: Cases with no linear boundaries."
   ],
   "id": "9d323127d4490d94"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
