
强化学习 (RL)

## 基本概念

策略 $\pi (a| s) = P(A = a|S = s)$, 在 $s$ 状态下做出 $a$ 动作的概率

两种随机: 动作策略上的随机性 $A \sim \pi (\cdot\ |\ S)$、状态转移过程的随机 $S' \sim P(\cdot\ |\ s, a)$

## Markov Decision Process (MDP)

**Non-Deterministic Search: Grid World**: 每一步不是确定的步骤，而是依概率的.

马尔可夫问题: $P(S_{t + 1} = s'|S_t = s, A_t = a, \ldots, A_0 = a_0, S_0 = s_0) = P(S_{t + 1} = s'|S_t = s, A_t = a)$

### Value Iteration

三个函数:  
- $V^*(s)$: 状态 $s$ 的价值  
- $Q^*(s, a)$: 从 $s$ 出发做行为 $a$ 后，未来按照**最优**策略继续行动能获得的**长期**回报  
- $\pi^*(s)$: 从 $s$ 出发的决策  
- $T(s, a, s')$: 状态转移函数, 采取动作 $a$ 后到达状态 $s'$ 的概率  
- $R(s, a, s')$: 做动作 $a$ 由 $s$ 到达 $s'$ 的即时奖励  
- $\gamma$: 折扣因子, 表示未来回报的重要程度

递归表达式 (**Bellman 最优方程** (Bellman Optimality Equation)):

$$
\begin{align}
    V^*(s) &= \max_a Q^*(s, a) \\
    Q^*(s, a) &= \sum_{s'}T(s, a, s')\left(R(s, a, s') + \gamma V^*(s')\right) \\
    \Rightarrow V_{k + 1}^*(s) &= \max_a \sum_s' T(s, a, s')\left(R(s, a, s') + \gamma V_k^*(s')\right)
\end{align}
$$

可以证明最后 $V_k$ 收敛到最优解.

### Policy Iteration

在决策 $\pi$ 下迭代更新 $V$

$$
V_{k + 1}^{\pi}(s) 
= \sum_{s'} T(s,\pi(s),s')\left( R(s,\pi(s),s') + \gamma V_k^{\pi}(s') \right)
$$

进行迭代获得比较收敛的 $V^{\pi_i}$ 后，更新 $\pi_i$:

$$
\pi_{i+1}(s)
= \argmax_a \sum_{s'} T(s,a,s')\left( R(s,a,s') + \gamma V^{\pi_i}(s') \right)
$$

也即使用现有 $\pi$ 更新 $V$, 然后用 $V$ 更新策略 $\pi$. 在一定情况下它收敛得比 value iteration 快很多.

## Q-Learning (for RL)

### Double-Bandit MDP
