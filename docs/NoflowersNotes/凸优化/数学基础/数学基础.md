
~~上学期数分学的忘光啦 赶紧 review 一下~~

## Derivative and Gradient

A function $\boldsymbol{f} : S \subset \mathbb{R}^n \to \mathbb{R}^m$ is 
differentiable at $\boldsymbol{x}_0 \in \operatorname{int} S$, if there exists 
a matrix $\boldsymbol{A} \in \mathbb{R}^{m \times n}$ such that

$$
\lim_{\Delta \boldsymbol{x} \to \boldsymbol{0}}
\frac{
\boldsymbol{f}(\boldsymbol{x}_0 + \Delta \boldsymbol{x}) 
- \boldsymbol{f}(\boldsymbol{x}_0)
- \boldsymbol{A}\,\Delta \boldsymbol{x}
}{
\lVert \Delta \boldsymbol{x} \rVert
}
= 0.
$$

i.e.

$$
\boldsymbol{f}(\boldsymbol{x})
=
\boldsymbol{f}(\boldsymbol{x}_0)
+
\boldsymbol{A}(\boldsymbol{x} - \boldsymbol{x}_0)
+
o(\lVert \boldsymbol{x} - \boldsymbol{x}_0 \rVert).
$$

Then we call the matrix $\boldsymbol{A}$ the derivative of $\boldsymbol{f}$ at $\boldsymbol{x}_0$, i.e.

$$
\boldsymbol{f}'(\boldsymbol{x}_0) = D\boldsymbol{f}(\boldsymbol{x}_0) = \boldsymbol{A}
$$

$$
[D\boldsymbol{f}(\boldsymbol{x}_0)]_{ij} = \frac{\partial f_i(\boldsymbol{x_0})}{\partial x_j}
$$

$$
f_i(\boldsymbol{x}_0 + \Delta \boldsymbol{x})
= f_i(\boldsymbol{x}_0)
+ \sum_{j=1}^{n}
    \frac{\partial f_i(\boldsymbol{x}_0)}{\partial x_j}\,
    \Delta x_j
+ o(\lVert \Delta \boldsymbol{x} \rVert),
\ i = 1, 2, \ldots, m
$$

Gradient is the transpose of derivative for a real valued function $\mathbb{R}^n \to \mathbb{R}$, which is a column vector.

$$
f'(\boldsymbol{x})\Delta \boldsymbol{x} = \langle \nabla f(\boldsymbol{x}), \boldsymbol{x} \rangle = \nabla f(\boldsymbol{x})^T \boldsymbol{x}
$$

!!! examples "Some examples"

    (来自上学期的线代笔记啦)

    $$
    \begin{align}
        &\frac{\mathrm{d}\boldsymbol{x}^T \boldsymbol{y}}{\mathrm{d}\boldsymbol{x}} = \frac{\mathrm{d}\boldsymbol{y}^T \boldsymbol{x}}{\mathrm{d}\boldsymbol{x}} = \boldsymbol{y} & \quad \text{The inner product} \\
        & \frac{\mathrm{d}\boldsymbol{x}^T \boldsymbol{Ax}}{\mathrm{d}\boldsymbol{x}} = (\boldsymbol{A} + \boldsymbol{A}^T)\boldsymbol{x} & \\
        & \frac{\mathrm{d}\ \mathrm{tr}(\boldsymbol{B}^T\boldsymbol{A})}{\mathrm{d}\boldsymbol{A}} = \boldsymbol{B} & \\
        & \frac{\mathrm{d}\ \mathrm{tr}(\boldsymbol{X}^T\boldsymbol{A}\boldsymbol{X})}{\mathrm{d}\boldsymbol{X}} = (\boldsymbol{A} + \boldsymbol{A}^T)\boldsymbol{X} & \\
        & \frac{\mathrm{d}\det \boldsymbol{}}{\mathrm{d}\boldsymbol{A}} = \det \boldsymbol{A} (\boldsymbol{A}^{-1})^T
    \end{align}
    $$

## Secondary derivative

The second-order partial derivatives of 
$f : S \subset \mathbb{R}^n \to \mathbb{R}$ at $\boldsymbol{x}_0 \in \operatorname{int} S$ are

$$
\frac{\partial^2 f(\boldsymbol{x}_0)}{\partial x_i \partial x_j}
\;\triangleq\;
\left.
\frac{\partial}{\partial x_i}
\left( \frac{\partial f(x)}{\partial x_j} \right)
\right|_{x = \boldsymbol{x}_0}
\quad i,j = 1,2,\ldots,n
$$

The Hessian (matrix) of $f$ at $\boldsymbol{x}_0$, denoted by 

$$
\bigl[ \boldsymbol{\nabla}^2 f(\boldsymbol{x}_0) \bigr]_{ij}
\;=\;
\frac{\partial^2 f(\boldsymbol{x}_0)}{\partial x_i \partial x_j}
\quad i,j = 1,2,\ldots,n
$$

The second-order Taylor expansion for $f : \mathbb{R}^n \to \mathbb{R}$ takes the form

$$
f(\boldsymbol{x} + \boldsymbol{d})
=
f(\boldsymbol{x})
+ \nabla f(\boldsymbol{x})^{T} \boldsymbol{d}
+ \frac{1}{2}
\boldsymbol{d}^{T}\nabla^{2} f(\boldsymbol{x})
\boldsymbol{d}
+ o(\lVert \boldsymbol{d} \rVert^{2})
$$

## Other appendixes

### Bounds on quadratic forms

For a symmetric matrix $\boldsymbol{A} \in \mathbb{R}^{n \times n}$,

$$
\lambda_{\min}\, \lVert \boldsymbol{x} \rVert_2^{2}
\;\le\;
\boldsymbol{x}^{T} \boldsymbol{A}\, \boldsymbol{x}
\;\le\;
\lambda_{\max}\, \lVert \boldsymbol{x} \rVert_2^{2},
\qquad
\forall\, \boldsymbol{x} \in \mathbb{R}^{n},
$$

where $\lambda_{\max}$ and $\lambda_{\min}$ are the largest and the smallest eigenvalues of $\boldsymbol{A}$.

### Second-order sufficient condition

If $f : \mathbb{R}^{n} \to \mathbb{R}$ is twice continuously differentiable and $\boldsymbol{x}^{*}$ is a local minimum of $f$, then its Hessian matrix $\boldsymbol{\nabla}^{2} f(\boldsymbol{x}^{*})$ is positive semidefinite, i.e.

$$
\boldsymbol{d}^{T}\,
\boldsymbol{\nabla}^{2} f(\boldsymbol{x}^{*})\,
\boldsymbol{d}
\;\ge\; 0,
\quad \forall\, \boldsymbol{d} \in \mathbb{R}^{n}
$$

