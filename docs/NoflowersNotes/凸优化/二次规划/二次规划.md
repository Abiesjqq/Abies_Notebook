
## Types of QP problems

### Quadratic Program (QP) and Quadratically constrained quadratic program (QCQP)

**QP**

$$
\begin{aligned}
    \min_x\  &\frac{1}{2}\boldsymbol{x}^T\boldsymbol{Q}\boldsymbol{x} + \boldsymbol{c}^T\boldsymbol{x} \\
    \text{s.t.}\ &\boldsymbol{Bx}\leq \boldsymbol{d} \\
    \ &\boldsymbol{Ax} = \boldsymbol{b}
\end{aligned}
$$

QP is convex iff $\boldsymbol{Q} \succeq \boldsymbol{O}$, and reduces to LP if $\boldsymbol{Q} = \boldsymbol{O}$

**QCQP**

$$
\begin{aligned}
    \min_x\  &\frac{1}{2}\boldsymbol{x}^T\boldsymbol{Q}\boldsymbol{x} + \boldsymbol{c}^T\boldsymbol{x} \\
    \text{s.t.}\ &\frac{1}{2}\boldsymbol{x}^T\boldsymbol{Q}_i\boldsymbol{x} + \boldsymbol{c}_i^T\boldsymbol{x}\leq \boldsymbol{d} + \boldsymbol{d}_i \leq 0 & i = 1, 2, \ldots, m \\
    \ &\boldsymbol{Ax} = \boldsymbol{b}
\end{aligned}
$$

QCQP is convex if $\boldsymbol{Q} \succeq \boldsymbol{O}$ and $\boldsymbol{Q}_i \succeq \boldsymbol{O},\ \forall i$, and reduced to QP if $\boldsymbol{Q}_i = \boldsymbol{O}$

!!! examples "Linear least squares regression"

    Given $\boldsymbol{y} \in \mathbb{R}^n, \boldsymbol{X} \in \mathbb{R}^{n \times p}$, find $\boldsymbol{w}\in \mathbb{R}^p$ s.t. 

    $$
    \min_{\boldsymbol{w}}\Vert \boldsymbol{y} - \boldsymbol{Xw} \Vert_2^2
    $$

    Geometrically, we are looking for the orthogonal projection $\hat{\boldsymbol{y}}$ of $\boldsymbol{y}$ onto $\operatorname{Range}(\boldsymbol{X})$, the column space of $\boldsymbol{X}$.

    Let $f(\boldsymbol{w}) = \boldsymbol{w}^T\boldsymbol{X}^T\boldsymbol{Xw} - 2\boldsymbol{y}^T\boldsymbol{Xw} + \boldsymbol{y}^T\boldsymbol{y}$, $\boldsymbol{w}^*$ is optimal iff.

    $$
    \nabla f(\boldsymbol{w}^*) = \boldsymbol{0} \Leftrightarrow \boldsymbol{X}^T\boldsymbol{Xw} = \boldsymbol{X}^T\boldsymbol{y}
    $$

    then $\boldsymbol{w}^*$ is unique iff. $\boldsymbol{X}^T\boldsymbol{X} \succ \boldsymbol{O}$ iff. $\boldsymbol{X}$ has full column rank, $\boldsymbol{w}^* = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}$

$\boldsymbol{x}^*$ is optimal iff

$$
\nabla f(\boldsymbol{x}) = \boldsymbol{Qx} + \boldsymbol{b} = \boldsymbol{0}
$$

Case 1. $\boldsymbol{Q}\succ \boldsymbol{O}$: unique solution $\boldsymbol{x}^* = -\boldsymbol{Q}^{-1}\boldsymbol{b}$  
Case 2. $\det \boldsymbol{Q} = 0$ and $\boldsymbol{b} \in \operatorname{Range}(\boldsymbol{Q})$: infinite solutions.  
Case 3. $\det \boldsymbol{Q} = 0$ and $\boldsymbol{b} \notin \operatorname{Range}(\boldsymbol{Q})$: no solutions and $f^* = -\infty$

![alt text](image.png)

!!! normal-comment "Why $\boldsymbol{b}$ and $\operatorname{Null}(\boldsymbol{Q})$ matters?"
    $$
    \operatorname{Range}(\boldsymbol{A}) = \operatorname{Null}(\boldsymbol{A})^\perp
    $$

!!! examples "Examples"

    **LOSSO** (Least Absolute Shrinkage and Selection Operator)

    Given $\boldsymbol{y} \in \mathbb{R}^n, \boldsymbol{X} \in \mathbb{R}^{n \times p}, t > 0$, 

    $$
    \begin{aligned}
        \min_{\boldsymbol{w}}\ &\Vert \boldsymbol{y} - \boldsymbol{Xw} \Vert_2^2 \\
        \text{s.t.}\ &\Vert \boldsymbol{w}\Vert_1 \leq t
    \end{aligned}
    $$

    The optimal solution always exists because continuousness, closeness and boundedness.  
    And the result is unique.

    **Ridge Regression**

    Given $\boldsymbol{y} \in \mathbb{R}^n, \boldsymbol{X} \in \mathbb{R}^{n \times p}, t > 0$, 

    $$
    \begin{aligned}
        \min_{\boldsymbol{w}}\ &\Vert \boldsymbol{y} - \boldsymbol{Xw} \Vert_2^2 \\
        \text{s.t.}\ &\Vert \boldsymbol{w}\Vert_2 \leq t
    \end{aligned}
    $$

    It is a QCQP

    **SVM**

    $$
    \begin{aligned}
        \min_{\boldsymbol{w}, b}\ &\frac{1}{2}\Vert\boldsymbol{w}\Vert^2 \\
        \mathrm{s.t.}\ & y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b) \geq 1
    \end{aligned}
    $$

### Geometric Program

A **monomial** is a function $f : \mathbb{R}^n_{++} = \{ \boldsymbol{x} \in \mathbb{R}^n : \boldsymbol{x} > \boldsymbol{0} \} \to \mathbb{R}$ of the form:

$$
f(\boldsymbol{x}) = \gamma x_1^{a_1} x_2^{a_2} \cdots x_n^{a_n}
$$

for $\gamma > 0$, $a_1, \ldots, a_n \in \mathbb{R}$  
A **posynomial** is a sum of monomials:

$$
f(\boldsymbol{x}) = \sum_{k=1}^{p} \gamma_k x_1^{a_{k1}} x_2^{a_{k2}} \cdots x_n^{a_{kn}}
$$

A **geometric program** (GP) is an optimization problem of the form 

$$
\begin{aligned}
    \min_{\boldsymbol{x}}\ & f(\boldsymbol{x}) \\
    \text{s.t.}\ & g_i(\boldsymbol{x}) \leq 1 & i = 1, \ldots, m \\
    \ & h_j(\boldsymbol{x}) = 1 & j = 1, \ldots, r
\end{aligned}
$$

where $f,\, g_i,\ i = 1,\ldots,m$ are posynomials and $h_j,\ j = 1,\ldots,r$ are monomials. The constraint $x > 0$ is implicit.


