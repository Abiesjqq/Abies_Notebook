
## Descent Method

### Basic method

$$
\boldsymbol{x}_{k + 1} = \boldsymbol{x}_k - t_k \nabla f(\boldsymbol{x}_k)
$$

$-\nabla f(\boldsymbol{x}_k)$ is the direction of fastest rate of decrease of $f$ at $\boldsymbol{x}_k$:  

!!! remarks "Proof"

    If $\lVert \boldsymbol{d}_k \rVert_2 = 1$,

    $$
    \lim_{t \to  0} 
    \frac{ f(\boldsymbol{x}_k) - f(\boldsymbol{x}_k + t\,\boldsymbol{d}_k) }{t}
    = -\, \boldsymbol{d}_k^{T} \nabla f(\boldsymbol{x}_k)
    \leq\lVert \nabla f(\boldsymbol{x}_k) \rVert_2
    $$

    with equality iff $\displaystyle\boldsymbol{d}_k= - \frac{ \nabla f(\boldsymbol{x}_k) }{ \lVert \nabla f(\boldsymbol{x}_k) \rVert_2 }$

!!! remarks "Lipschitz Continuity"
    A function $\boldsymbol{f}: \mathbb{R}^n \to \mathbb{R}^m$ is Lipschitz continuous iff there exists a constant $C > 0$ s.t.

    $$
    \Vert \boldsymbol{f}(\boldsymbol{x}) - \boldsymbol{f}(\boldsymbol{y}) \Vert \leq C \Vert \boldsymbol{x} - \boldsymbol{y} \Vert
    $$

    !!! examples "Examples"

        $\boldsymbol{f}(\boldsymbol{x}) = \boldsymbol{Qx}$ is $\sigma_\text{max}(\boldsymbol{Q})$-Lipschitz:  
        Let $\boldsymbol{x} - \boldsymbol{y} = \boldsymbol{d}$,

        $$
        \Vert \boldsymbol{f}(\boldsymbol{x}) - \boldsymbol{f}(\boldsymbol{y}) \Vert^2 = \boldsymbol{d}^T\boldsymbol{Q}^T\boldsymbol{Q}\boldsymbol{d} \leq \lambda_\text{max}(\boldsymbol{Q}^T\boldsymbol{Q})\Vert \boldsymbol{d} \Vert^2
        $$

!!! remarks "$L$-smoothness"
    A function is $L$-smooth if it is differentiable and ist gradiend is $L$-Lipschitz, i.e.

    $$
    \Vert \nabla\boldsymbol{f}(\boldsymbol{x}) - \nabla\boldsymbol{f}(\boldsymbol{y}) \Vert \leq C \Vert \boldsymbol{x} - \boldsymbol{y} \Vert
    $$

    !!! examples "Examples"
        $f(\boldsymbol{x}) = \boldsymbol{x}^T\boldsymbol{Q}\boldsymbol{x}$ with $\boldsymbol{Q} \succeq \boldsymbol{O}$ is $\sigma_\text{max}(\boldsymbol{Q})$-smooth.

    A twice continuously differentiable convex $f: \mathbb{R}^n \to \mathbb{R}$ is $L$-smooth iff 

    $$
    \nabla^2f(\boldsymbol{x}) \preceq L \boldsymbol{I} \Leftrightarrow \lambda_\text{max}(\nabla^2f(\boldsymbol{x})) \leq L
    $$

#### Quadratic upper bound

If $f$ is $L$-smooth, then

$$
f(\boldsymbol{y}) \leq f(\boldsymbol{x}) + \nabla f(\boldsymbol{x})^T(\boldsymbol{y} - \boldsymbol{x}) + \frac{L}{2}\Vert \boldsymbol{y} - \boldsymbol{x}\Vert^2
$$

![alt text](image.png)

!!! remarks "Proof"

    First prove the 1D case. Let $g(t)$ be $L_g$-smooth, $\lvert g'(t)-g'(s)\rvert\le L_g\lvert t-s\rvert$.

    $$
    \begin{aligned}
    g(1)
    &= g(0)+\int_{0}^{1} g'(t)\mathrm{d}t \\
    &= g(0)+g'(0)+\int_{0}^{1}\bigl[g'(t)-g'(0)\bigr]\mathrm{d}t \\
    &\leq g(0)+g'(0)+\int_{0}^{1}L_g t\mathrm{d}t
    \quad\text{since }\lvert g'(t)-g'(0)\rvert\leq L_g t \\
    &= g(0)+g'(0)+\frac12 L_g
    \end{aligned}
    $$

    For the general case, apply the above to $g(t)=f(\boldsymbol{x}+t\boldsymbol{d})$ with $\boldsymbol{d}=\boldsymbol{y}-\boldsymbol{x}$ and $L_g = L \lVert \boldsymbol{d}\rVert^{2}$. To check the smoothness of $g$,

    $$
    \begin{aligned}
    \lvert g'(t)-g'(s)\rvert
    &= \bigl\lvert [\nabla f(\boldsymbol{x}+t\boldsymbol{d}) - \nabla f(\boldsymbol{x}+s\boldsymbol{d})]^T \boldsymbol{d} \bigr\rvert \\
    &\le \lVert \nabla f(\boldsymbol{x}+t\boldsymbol{d}) - \nabla f(\boldsymbol{x}+s\boldsymbol{d}) \rVert \lVert \boldsymbol{d} \rVert 
    \quad&\text{Cauchy--Schwarz} \\
    &\le (t-s) L \lVert \boldsymbol{d}\rVert^{2} \quad &f \text{ is $L$-smooth}.
    \end{aligned}
    $$

**Consequence of quadratic upper bound**

For $L$-smooth $f$, the sequence $\{\boldsymbol{x}_k\}$ produced by gradient descent satisfies:

$$
f(\boldsymbol{x}_{k + 1}) \leq f(\boldsymbol{x}_k) - t\left(1 - \frac{L}{2}t\right) \Vert \nabla f(\boldsymbol{x}_k) \Vert^2
$$

!!! remarks "Proof"
    Using the theorem above and $\nabla f(\boldsymbol{x})^T(\boldsymbol{x}_{k + 1} - \boldsymbol{x}_k) = - t \Vert \nabla f(\boldsymbol{x}_k) \Vert^2$.

i.e. 

$$
f(\boldsymbol{x}_{k}) - f(\boldsymbol{x}_{k + 1})  \geq  t\left(1 - \frac{L}{2}t\right) \Vert \nabla f(\boldsymbol{x}_k) \Vert^2 \geq \frac{1}{2}t \Vert \nabla f(\boldsymbol{x}_k) \Vert^2
$$

if $\displaystyle 0 \leq t \leq \frac{1}{L}$ (**Descent Lemma**)

**Convergence Analysis**

If step size $t \in (0, 1 / L]$, and $\boldsymbol{x}^*$ is a minimum of $f$, then 

$$
f(\boldsymbol{x}_k) - f(\boldsymbol{x}^*) \leq \frac{\Vert \boldsymbol{x}_0 - \boldsymbol{x}^*\Vert^2}{2tk}
$$

!!! normal-comment "Notes"
    $f(x) \to f^*$ as $k \to \infty$  
    Any limiting point of $\boldsymbol{x}_k$ is an optimal solution
    The rate of convergence if $O(1/k)$, i.e. number of iterations to guarantee $f(\boldsymbol{x}_k) - f^* \leq \varepsilon$ is $O(1 / \varepsilon)$, **exponential**!  
    Faster convergence with larger $t$; best $t = 1 / L$, but $L$ is unknown

!!! remarks "Proof"



### Gradient Flow

A continuous version of gradient descent.

$$
\frac{\mathrm{d}}{\mathrm{d}s}\tilde{\boldsymbol{x}}_s = -\nabla f(\tilde{\boldsymbol{x}}_s)
$$

at time $s$.

We also have:

If $f$ is convex and $\boldsymbol{x}^*$ is the minimum of $f$

$$
f(\tilde{\boldsymbol{x}}_T) - f(\boldsymbol{x}^*) 
\le \frac{\|\tilde{\boldsymbol{x}}_0 - \boldsymbol{x}^*\|^2}{2T}
$$

at time $T$

!!! remarks "Proof"

    $$
    \begin{aligned}
        \frac{\mathrm d}{\mathrm ds}\|\tilde{\boldsymbol{x}}_s - \boldsymbol{x}^*\|^2
        &= 2\langle \tilde{\boldsymbol{x}}_s - \boldsymbol{x}^*, 
        \frac{\mathrm d}{\mathrm ds}\tilde{\boldsymbol{x}}_s\rangle
        = 2\langle \nabla f(\tilde{\boldsymbol{x}}_s), \boldsymbol{x}^* - \tilde{\boldsymbol{x}}_s\rangle \\
        &\leq 2\bigl(f(\boldsymbol{x}^*) - f(\tilde{\boldsymbol{x}}_s)\bigr)
    \end{aligned}
    $$

    Integrating from $0$ to $T$ and using $f(\tilde{\boldsymbol{x}}_s) \ge f(\tilde{\boldsymbol{x}}_T)$,

    $$
    \begin{aligned}
        \|\tilde{\boldsymbol{x}}_T - \boldsymbol{x}^*\|^2
        - \|\tilde{\boldsymbol{x}}_0 - \boldsymbol{x}^*\|^2
        &\leq \int_0^T 2\bigl(f(\boldsymbol{x}^*) - f(\tilde{\boldsymbol{x}}_s)\bigr)\mathrm ds \\
        &\leq 2T\bigl(f(\boldsymbol{x}^*) - f(\tilde{\boldsymbol{x}}_T)\bigr)
        \end{aligned}
    $$
