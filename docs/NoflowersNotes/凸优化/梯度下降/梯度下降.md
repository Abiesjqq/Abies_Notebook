
## Descent Method

### Basic method

$$
\boldsymbol{x}_{k + 1} = \boldsymbol{x}_k - t_k \nabla f(\boldsymbol{x}_k)
$$

$-\nabla f(\boldsymbol{x}_k)$ is the direction of fastest rate of decrease of $f$ at $\boldsymbol{x}_k$:  

!!! remarks "Proof"

    If $\lVert \boldsymbol{d}_k \rVert_2 = 1$,

    $$
    \lim_{t \to  0} 
    \frac{ f(\boldsymbol{x}_k) - f(\boldsymbol{x}_k + t\,\boldsymbol{d}_k) }{t}
    = -\, \boldsymbol{d}_k^{T} \nabla f(\boldsymbol{x}_k)
    \leq\lVert \nabla f(\boldsymbol{x}_k) \rVert_2
    $$

    with equality iff $\displaystyle\boldsymbol{d}_k= - \frac{ \nabla f(\boldsymbol{x}_k) }{ \lVert \nabla f(\boldsymbol{x}_k) \rVert_2 }$

!!! remarks "Lipschitz Continuity"
    A function $\boldsymbol{f}: \mathbb{R}^n \to \mathbb{R}^m$ is Lipschitz continuous iff there exists a constant $C > 0$ s.t.

    $$
    \Vert \boldsymbol{f}(\boldsymbol{x}) - \boldsymbol{f}(\boldsymbol{y}) \Vert \leq C \Vert \boldsymbol{x} - \boldsymbol{y} \Vert
    $$

    !!! examples "Examples"

        $\boldsymbol{f}(\boldsymbol{x}) = \boldsymbol{Qx}$ is $\sigma_\text{max}(\boldsymbol{Q})$-Lipschitz:  
        Let $\boldsymbol{x} - \boldsymbol{y} = \boldsymbol{d}$,

        $$
        \Vert \boldsymbol{f}(\boldsymbol{x}) - \boldsymbol{f}(\boldsymbol{y}) \Vert^2 = \boldsymbol{d}^T\boldsymbol{Q}^T\boldsymbol{Q}\boldsymbol{d} \leq \lambda_\text{max}(\boldsymbol{Q}^T\boldsymbol{Q})\Vert \boldsymbol{d} \Vert^2
        $$

!!! remarks "$L$-smoothness"
    A function is $L$-smooth if it is differentiable and ist gradiend is $L$-Lipschitz, i.e.

    $$
    \Vert \nabla\boldsymbol{f}(\boldsymbol{x}) - \nabla\boldsymbol{f}(\boldsymbol{y}) \Vert \leq C \Vert \boldsymbol{x} - \boldsymbol{y} \Vert
    $$

    !!! examples "Examples"
        $f(\boldsymbol{x}) = \boldsymbol{x}^T\boldsymbol{Q}\boldsymbol{x}$ with $\boldsymbol{Q} \succeq \boldsymbol{O}$ is $\sigma_\text{max}(\boldsymbol{Q})$-smooth.

    A twice continuously differentiable convex $f: \mathbb{R}^n \to \mathbb{R}$ is $L$-smooth iff 

    $$
    \nabla^2f(\boldsymbol{x}) \preceq L \boldsymbol{I} \Leftrightarrow \lambda_\text{max}(\nabla^2f(\boldsymbol{x})) \leq L
    $$

#### Quadratic upper bound

If $f$ is $L$-smooth, then

$$
f(\boldsymbol{y}) \leq f(\boldsymbol{x}) + \nabla f(\boldsymbol{x})^T(\boldsymbol{y} - \boldsymbol{x}) + \frac{L}{2}\Vert \boldsymbol{y} - \boldsymbol{x}\Vert^2
$$

![alt text](image.png)

!!! remarks "Proof"

    First prove the 1D case. Let $g(t)$ be $L_g$-smooth, $\lvert g'(t)-g'(s)\rvert\le L_g\lvert t-s\rvert$.

    $$
    \begin{aligned}
    g(1)
    &= g(0)+\int_{0}^{1} g'(t)\mathrm{d}t \\
    &= g(0)+g'(0)+\int_{0}^{1}\bigl[g'(t)-g'(0)\bigr]\mathrm{d}t \\
    &\leq g(0)+g'(0)+\int_{0}^{1}L_g t\mathrm{d}t
    \quad\text{since }\lvert g'(t)-g'(0)\rvert\leq L_g t \\
    &= g(0)+g'(0)+\frac12 L_g
    \end{aligned}
    $$

    For the general case, apply the above to $g(t)=f(\boldsymbol{x}+t\boldsymbol{d})$ with $\boldsymbol{d}=\boldsymbol{y}-\boldsymbol{x}$ and $L_g = L \lVert \boldsymbol{d}\rVert^{2}$. To check the smoothness of $g$,

    $$
    \begin{aligned}
    \lvert g'(t)-g'(s)\rvert
    &= \bigl\lvert [\nabla f(\boldsymbol{x}+t\boldsymbol{d}) - \nabla f(\boldsymbol{x}+s\boldsymbol{d})]^T \boldsymbol{d} \bigr\rvert \\
    &\le \lVert \nabla f(\boldsymbol{x}+t\boldsymbol{d}) - \nabla f(\boldsymbol{x}+s\boldsymbol{d}) \rVert \lVert \boldsymbol{d} \rVert 
    \quad&\text{Cauchy--Schwarz} \\
    &\le (t-s) L \lVert \boldsymbol{d}\rVert^{2} \quad &f \text{ is $L$-smooth}.
    \end{aligned}
    $$

**Consequence of quadratic upper bound**

For $L$-smooth $f$, the sequence $\{\boldsymbol{x}_k\}$ produced by gradient descent satisfies:

$$
f(\boldsymbol{x}_{k + 1}) \leq f(\boldsymbol{x}_k) - t\left(1 - \frac{L}{2}t\right) \Vert \nabla f(\boldsymbol{x}_k) \Vert^2
$$

!!! remarks "Proof"
    Using the theorem above and $\nabla f(\boldsymbol{x})^T(\boldsymbol{x}_{k + 1} - \boldsymbol{x}_k) = - t \Vert \nabla f(\boldsymbol{x}_k) \Vert^2$.

i.e. 

$$
f(\boldsymbol{x}_{k}) - f(\boldsymbol{x}_{k + 1})  \geq  t\left(1 - \frac{L}{2}t\right) \Vert \nabla f(\boldsymbol{x}_k) \Vert^2 \geq \frac{1}{2}t \Vert \nabla f(\boldsymbol{x}_k) \Vert^2
$$

if $\displaystyle 0 \leq t \leq \frac{1}{L}$ (**Descent Lemma**)

**Convergence Analysis**

If step size $t \in (0, 1 / L]$, and $\boldsymbol{x}^*$ is a minimum of $f$, then 

$$
f(\boldsymbol{x}_k) - f(\boldsymbol{x}^*) \leq \frac{\Vert \boldsymbol{x}_0 - \boldsymbol{x}^*\Vert^2}{2tk}
$$

!!! normal-comment "Notes"
    $f(x) \to f^*$ as $k \to \infty$  
    Any limiting point of $\boldsymbol{x}_k$ is an optimal solution
    The rate of convergence if $O(1/k)$, i.e. number of iterations to guarantee $f(\boldsymbol{x}_k) - f^* \leq \varepsilon$ is $O(1 / \varepsilon)$, **exponential**!  
    Faster convergence with larger $t$; best $t = 1 / L$, but $L$ is unknown

!!! remarks "Proof"

    $$
    \begin{aligned}
        \|\boldsymbol{x}_{k+1} - \boldsymbol{x}^*\|^2 
        - \|\boldsymbol{x}_k - \boldsymbol{x}^*\|^2
        &= \|\boldsymbol{x}_k - t\nabla f(\boldsymbol{x}_k) - \boldsymbol{x}^*\|^2
        - \|\boldsymbol{x}_k - \boldsymbol{x}^*\|^2 \\
        &= 2t \nabla f(\boldsymbol{x}_k)^T (\boldsymbol{x}^* - \boldsymbol{x}_k) 
        + t^2 \|\nabla f(\boldsymbol{x}_k)\|^2 \\
        & \leq 2t \bigl[f(\boldsymbol{x}^*) - f(\boldsymbol{x}_k)\bigr] + 2t\bigl[f(\boldsymbol{x}_k) - f(\boldsymbol{x}_{k+1})\bigr] \\
        &= 2t\bigl[f(\boldsymbol{x}^*) - f(\boldsymbol{x}_{k+1})\bigr] \\
    \end{aligned}
    $$

    Sum over $k$ from $0$ to $N-1$,

    $$
    \begin{aligned}
        \|\boldsymbol{x}_N - \boldsymbol{x}^*\|^2
        - \|\boldsymbol{x}_0 - \boldsymbol{x}^*\|^2
        &\leq 2t \sum_{k=0}^{N-1} \bigl[f(\boldsymbol{x}^*) - f(\boldsymbol{x}_{k+1})\bigr] \\
        &\leq 2tN \bigl[f(\boldsymbol{x}^*) - f(\boldsymbol{x}_N)\bigr]
    \end{aligned}
    $$

    So 

    $$
    f(\boldsymbol{x}_N) - f(\boldsymbol{x}^*)
    \le \frac{\|\boldsymbol{x}_0 - \boldsymbol{x}^*\|^2}{2tN}
    $$

### Gradient Flow

A continuous version of gradient descent.

$$
\frac{\mathrm{d}}{\mathrm{d}s}\tilde{\boldsymbol{x}}_s = -\nabla f(\tilde{\boldsymbol{x}}_s)
$$

at time $s$.

We also have:

If $f$ is convex and $\boldsymbol{x}^*$ is the minimum of $f$

$$
f(\tilde{\boldsymbol{x}}_T) - f(\boldsymbol{x}^*) 
\le \frac{\|\tilde{\boldsymbol{x}}_0 - \boldsymbol{x}^*\|^2}{2T}
$$

at time $T$

!!! remarks "Proof"

    $$
    \begin{aligned}
        \frac{\mathrm d}{\mathrm ds}\|\tilde{\boldsymbol{x}}_s - \boldsymbol{x}^*\|^2
        &= 2\langle \tilde{\boldsymbol{x}}_s - \boldsymbol{x}^*, 
        \frac{\mathrm d}{\mathrm ds}\tilde{\boldsymbol{x}}_s\rangle
        = 2\langle \nabla f(\tilde{\boldsymbol{x}}_s), \boldsymbol{x}^* - \tilde{\boldsymbol{x}}_s\rangle \\
        &\leq 2\bigl(f(\boldsymbol{x}^*) - f(\tilde{\boldsymbol{x}}_s)\bigr)
    \end{aligned}
    $$

    Integrating from $0$ to $T$ and using $f(\tilde{\boldsymbol{x}}_s) \ge f(\tilde{\boldsymbol{x}}_T)$,

    $$
    \begin{aligned}
        \|\tilde{\boldsymbol{x}}_T - \boldsymbol{x}^*\|^2
        - \|\tilde{\boldsymbol{x}}_0 - \boldsymbol{x}^*\|^2
        &\leq \int_0^T 2\bigl(f(\boldsymbol{x}^*) - f(\tilde{\boldsymbol{x}}_s)\bigr)\mathrm ds \\
        &\leq 2T\bigl(f(\boldsymbol{x}^*) - f(\tilde{\boldsymbol{x}}_T)\bigr)
        \end{aligned}
    $$

### Strong Convexity

A function $f$ is strongly convex with parameter $m > 0$, or simply
**$m$-strongly convex**, if

$$
\tilde{f}(\boldsymbol{x}) = f(\boldsymbol{x}) - \frac{m}{2}\Vert \boldsymbol{x}\Vert^2
$$

is convex.

#### First order condition for strongly convex

A differentiable $f$ is $m$-strongly convex iff.

$$
f(\boldsymbol{y})\geq f(\boldsymbol{x}) + \nabla f(\boldsymbol{x})^T(\boldsymbol{y} - \boldsymbol{x}) + \frac{m}{2}\Vert \boldsymbol{x} - \boldsymbol{y}\Vert^2
$$

!!! remarks "Proof"

    $$
    \begin{aligned}
    f(\boldsymbol{y})
    &\ge f(\boldsymbol{x})
    + \nabla f(\boldsymbol{x})^{\!T}(\boldsymbol{y}-\boldsymbol{x})
    + \frac{m}{2}\bigl(\|\boldsymbol{y}\|^2-\|\boldsymbol{x}\|^2\bigr)
    - m\boldsymbol{x}^{T}(\boldsymbol{y}-\boldsymbol{x}) \\
    &= f(\boldsymbol{x})
    + \nabla f(\boldsymbol{x})^{\!T}(\boldsymbol{y}-\boldsymbol{x})
    + \frac{m}{2}\|\boldsymbol{y}-\boldsymbol{x}\|^2,
    \end{aligned}
    $$

#### Second order condition for strongly convex

A twice differentiable $f$ is $m$-strongly convex iff.

$$
\nabla^2 f(\boldsymbol{x}) \succeq m\boldsymbol{I} \Longleftrightarrow \lambda_{\text{min}}(\nabla^2 f(\boldsymbol{x})) \geq m
$$


!!! remarks "The bound of the strongly convex function"

    $m$-strong convexity and $L$-smoothness together imply

    $$
    \frac{m}{2}\,\|\boldsymbol{x}-\boldsymbol{y}\|^{2}\leq
    f(\boldsymbol{y})-f(\boldsymbol{x})-\nabla f(\boldsymbol{x})^{T}(\boldsymbol{y}-\boldsymbol{x})\leq
    \frac{L}{2}\,\|\boldsymbol{x}-\boldsymbol{y}\|^{2}
    $$

    i.e. (more precisely)

    $$
    \begin{aligned}
        \frac{1}{2}\lambda_{\text{min}}(\nabla^2 f(\boldsymbol{x}))\,\|\boldsymbol{x}-\boldsymbol{y}\|^{2}&\leq
        f(\boldsymbol{y})-f(\boldsymbol{x})-\nabla f(\boldsymbol{x})^{T}(\boldsymbol{y}-\boldsymbol{x})\\&\leq 
        \frac{1}{2}\lambda_{\text{max}}(\nabla^2 f(\boldsymbol{x}))\,\|\boldsymbol{x}-\boldsymbol{y}\|^{2}
    \end{aligned}
    $$

If $f$ is $m$-strongly convex, then

$$
f(\boldsymbol{x}) - f(\boldsymbol{x}^*) \leq \frac{1}{2m}\Vert \nabla f(\boldsymbol{x}) \Vert^2
$$

!!! remarks "Proof"

    $$
    \begin{aligned}
        f(\boldsymbol{x}^*) &= \min_{\boldsymbol{y}}f(\boldsymbol{y}) \geq \min_{\boldsymbol{y}} f(\boldsymbol{x}) + \nabla f(\boldsymbol{x})^T(\boldsymbol{y} - \boldsymbol{x}) + \frac{m}{2}\Vert \boldsymbol{x} - \boldsymbol{y}\Vert^2 \\
        &\geq f(\boldsymbol{x}) - \frac{1}{2m}\Vert \nabla f(\boldsymbol{x})\Vert^2\qquad \left(\text{Let }y = \boldsymbol{x} - \frac{1}{m}\nabla f(\boldsymbol{x})\right)
    \end{aligned}
    $$

#### The convergence of strongly convex in gradient flow

In this case we can know that

$$
\begin{aligned}
\frac{\mathrm{d}}{\mathrm{d}s}\,\|\boldsymbol{\tilde{x}}_{s}-\boldsymbol{x}^*\|^{2}
&= 2\langle \boldsymbol{\tilde{x}}_{s}-\boldsymbol{x}^*,\, \frac{\mathrm{d}}{\mathrm{d}s}\boldsymbol{\tilde{x}}_{s} \rangle \\
&= 2\left\langle \nabla f(\boldsymbol{\tilde{x}}_{s}),\, \boldsymbol{x}^* - \boldsymbol{\tilde{x}}_{s} \right\rangle \\
&\le 2\bigl[f(\boldsymbol{x}^*) - f(\boldsymbol{\tilde{x}}_{s})\bigr]
    - \frac{m}{2}\|\boldsymbol{\tilde{x}}_{s}-\boldsymbol{x}^*\|^{2} \\
&\le -m\|\boldsymbol{\tilde{x}}_{s}-\boldsymbol{x}^*\|^{2}.
\end{aligned}
$$

Then 

$$
\Vert \tilde{\boldsymbol{x}}_T - \boldsymbol{x}^*\Vert^2 \leq \mathrm{e}^{-mT}\Vert \tilde{\boldsymbol{x}}_0 - \boldsymbol{x}^*\Vert^2
$$

!!! normal-comment "The inequality"

    $$
    \begin{aligned}
        \displaystyle \frac{\mathrm{d}}{\mathrm{d}x}f(x) \leq Cf(x) &\Leftrightarrow [\mathrm{e}^{Cx} f(x)]' \leq 0 \Leftrightarrow \mathrm{e}^{Cx}f(x) \leq f(0) \\
        &\Leftrightarrow f(x) \leq \mathrm{e}^{-Cx}f(0)
    \end{aligned}
    $$

**Theorem**: If $f$ is $m$-strongly convex and $L$-smooth, and $\boldsymbol{x}^*$ is a minimum of $f$, then for step size $\displaystyle t \in (1, 1/L]$, the sequence ${\boldsymbol{x}_k}$ produced by the gradient descent algorithm satisfies:

$$
f(\boldsymbol{x}_k)-f(\boldsymbol{x}^*)
\le (1-mt)^{k}\bigl[f(\boldsymbol{x}_0)-f(\boldsymbol{x}^*)\bigr]
$$

$$
\|\boldsymbol{x}_k-\boldsymbol{x}^*\|^{2}
\le (1-mt)^{k}\|\boldsymbol{x}_0-\boldsymbol{x}^*\|^{2}.
$$

!!! normal-comment "Notes"

    The number of iterations to reach $f(\boldsymbol{x}_k) - f(\boldsymbol{x}^*) \leq \varepsilon$ is $O(\log (1 / \varepsilon))$, linear in the number of significant digits!  
    And the bound yield:

    $$
    \frac{m}{2}\,\|\boldsymbol{x}_k-\boldsymbol{x}^*\|^{2}\leq
    f(\boldsymbol{x}_k)-f(\boldsymbol{x}^*)\leq
    \frac{L}{2}\,\|\boldsymbol{x}_k-\boldsymbol{x}^*\|^{2}
    $$

!!! remarks "Proof"

    Similarly 

    $$
    \begin{aligned}
        \|\boldsymbol{x}_{k+1} - \boldsymbol{x}^*\|^2 
    - \|\boldsymbol{x}_k - \boldsymbol{x}^*\|^2
    &= 2t \nabla f(\boldsymbol{x}_k)^T (\boldsymbol{x}^* - \boldsymbol{x}_k) 
    + t^2 \|\nabla f(\boldsymbol{x}_k)\|^2 \\
    \end{aligned}
    $$

    this time, we have

    $$
    \nabla f(\boldsymbol{x}_k)^{T}(\boldsymbol{x}^*-\boldsymbol{x}_k)
    \le f(\boldsymbol{x}^*) - f(\boldsymbol{x}_k)
    - \frac{m}{2}\|\boldsymbol{x}_k-\boldsymbol{x}^*\|^{2}
    $$

    So

    $$
    \begin{aligned}
        \|\boldsymbol{x}_{k+1}-\boldsymbol{x}^*\|^{2}
    &\le (1-mt)\|\boldsymbol{x}_{k}-\boldsymbol{x}^*\|^{2}
    + 2t\bigl[f(\boldsymbol{x}^*)-f(\boldsymbol{x}_{k+1})\bigr] \\
    &\le (1-mt)\|\boldsymbol{x}_{k}-\boldsymbol{x}^*\|^{2}
    \end{aligned}
    $$

    By $L$-smoothness, 

    $$
    f(\boldsymbol{x}_k) - f(\boldsymbol{x}_{k + 1}) \geq \frac{t}{2}\Vert \nabla f(\boldsymbol{x}_k)\Vert^2
    $$

    By $m$-strong convexity, 

    $$
    f(\boldsymbol{x}_k) - f(\boldsymbol{x}^*) \leq  \frac{1}{2m}\Vert \nabla f(\boldsymbol{x}_k)\Vert^2
    $$

    eliminate $\Vert \nabla f(\boldsymbol{x}_k)\Vert^2$, we get

    $$
    f(\boldsymbol{x}_{k + 1}) - f(\boldsymbol{x}^*) \leq (1 - mt)[f(\boldsymbol{x}_{k}) - f(\boldsymbol{x}^*)]
    $$

## Condition number and Enhanced methods

For a quadratic function $\displaystyle f(\boldsymbol{x}) = \frac{1}{2}\boldsymbol{x}^T\boldsymbol{Qx}$, we introduce a number to depict its converging speed: **Condition number**

### Condition number

For a matrix $\boldsymbol{Q} \in \mathbb{R}^{n \times n}$ s.t. $\boldsymbol{Q} \succeq \boldsymbol{O}$, its condition number is defined as 

$$
\kappa(\boldsymbol{Q}) = \frac{\lambda_{\text{max}}(\boldsymbol{Q})}{\lambda_{\text{min}}(\boldsymbol{Q})}
$$

It characterizes how stretched the level curves of $\displaystyle f(\boldsymbol{x}) = \frac{1}{2}\boldsymbol{x}^T\boldsymbol{Qx}$ are.  
For nonquadratic case, $\kappa(\nabla^2 f(\boldsymbol{x}))$ plays a similar role.

A problem $\displaystyle \min_{\boldsymbol{x}}\frac{1}{2}\boldsymbol{x}^T\boldsymbol{Qx}$ is **well conditioned** if $\kappa(\boldsymbol{Q})$ is small. Or it is **ill conditioned**.

### Exact line search

Let $\displaystyle t_k = \arg\min_s f(\boldsymbol{x}_k - s\nabla f(\boldsymbol{x}_k))$ in each step!  
In practice it is expensive!

!!! examples "Examples"

    ![alt text](image-1.png)

    It does not guarantee a fast convergence!

!!! remarks "Convergence analysis"

    $$
    f(\boldsymbol{x}_k)-f(\boldsymbol{x}^*)
    \le \left(1-\frac{m}{L}\right)^{k}
    \bigl[f(\boldsymbol{x}_0)-f(\boldsymbol{x}^*)\bigr]
    $$

    Proof omitted for its similar as before.

### Backtracking ling search

Let $\alpha, \beta \in (0, 1)$ are constants, and in each step, do

$$
t \gets \beta t
$$

until

$$
f(\boldsymbol{x} - t\nabla f(\boldsymbol{x})) \leq f(\boldsymbol{x}) - \alpha t \Vert \nabla f(\boldsymbol{x})\Vert^2
$$

(**Armijo’s rule**)

!!! remarks "Noting"
    For general $\boldsymbol{d}$, use $f(\boldsymbol{x} + t \boldsymbol{d}) > f(\boldsymbol{x}) + \alpha t \nabla f(\boldsymbol{x})^T \boldsymbol{d}$

    ![alt text](image-2.png)

    In this image, $t_2$ is acceptable.

However, it still converges slow in ill-conditioned cases.

!!! normal-comment "Convergence Analysis"

    The sequence $\{\boldsymbol{x}_k\}$ satisfies:

    $$
    f(\boldsymbol{x}_k) - f(\boldsymbol{x}^*) \leq c^k[f(\boldsymbol{x}_0) - f(\boldsymbol{x}^*)]
    $$

    where

    $$
    c = 1 - \min \left\{2m\alpha t_0, \frac{4m\beta \alpha (1 - \alpha)}{L}\right\}
    $$

!!! remarks "Proof"

    $$
    f(\boldsymbol{x}_k - t \nabla f(\boldsymbol{x}_k))
    \le
    f(\boldsymbol{x}_k) - t \left(1 - \frac{L t}{2}\right) \lVert \nabla f(\boldsymbol{x}_k) \rVert^2
    $$

    The inner loop terminates for sure if (not sufficient)

    $$
    - t \left(1 - \frac{L t}{2}\right) \lVert \nabla f(\boldsymbol{x}_k) \rVert^2
    \le
    - \alpha t \lVert \nabla f(\boldsymbol{x}_k) \rVert^2
    \Longrightarrow
    t \le \frac{2(1-\alpha)}{L}
    $$

    and the former iteration doesn't terminate, 

    $$
    t_{k - 1} > \frac{2(1-\alpha)}{L} \Longrightarrow t_k > \frac{2\beta(1-\alpha)}{L}
    $$

    i.e.

    $$
    t_k \ge \eta \triangleq \min \left\{ t_0,\ \frac{2\beta(1-\alpha)}{L} \right\}
    $$

    Then 

    $$
    \begin{aligned}
        f(\boldsymbol{x}_{k+1}) - f(x^*) &\leq f(\boldsymbol{x}_k) - f(x^*) - \alpha t_k \lVert \nabla f(\boldsymbol{x}_k) \rVert^2 \\
        &\le f(\boldsymbol{x}_k) - f(x^*) - \alpha \eta \lVert \nabla f(\boldsymbol{x}_k) \rVert^2 \\
        \Longleftrightarrow & \lVert \nabla f(\boldsymbol{x}_k) \rVert^2
    \ge 2m \bigl[f(\boldsymbol{x}_k) - f(x^*)\bigr]
    \end{aligned}
    $$

    Eliminate $\lVert \nabla f(\boldsymbol{x}_k) \rVert^2$ and we get

    $$
    f(\boldsymbol{x}_k) - f(x^*)
    \le c^k \bigl[f(\boldsymbol{x}_0) - f(x^*)\bigr]
    $$

### Newton's method

Do better with second order derivative

Step $\boldsymbol{x}_{k + 1} = \boldsymbol{x}_k - t_k \nabla f(\boldsymbol{x}_k)$ can be interpreted as minimizing a quadratic approximation:

$$
\hat{f}_{\mathrm{gd}}(\boldsymbol{x}) \triangleq f(\boldsymbol{x}_k) + \nabla f(\boldsymbol{x})^T(\boldsymbol{x} - \boldsymbol{x}_k) + \frac{1}{2t_k}\Vert \boldsymbol{x} - \boldsymbol{x}_k \Vert^2
$$

And Newton's method minimizes the second-order Taylor approximation

$$
f(\boldsymbol{x}) \approx \hat{f}_{\mathrm{nt}}(\boldsymbol{x})
\triangleq
f(\boldsymbol{x}_k)
+ \nabla f(\boldsymbol{x}_k)^{T} (\boldsymbol{x} - \boldsymbol{x}_k)
+ \frac{1}{2} (\boldsymbol{x} - \boldsymbol{x}_k)^{T} \nabla^{2} f(\boldsymbol{x}_k) (\boldsymbol{x} - \boldsymbol{x}_k)
$$

So assuming $\nabla^2 f(\boldsymbol{x}_k) \succ \boldsymbol{O}$

$$
\boldsymbol{x}_{k + 1} = \boldsymbol{x}_k - \bigl[\nabla^2 f(\boldsymbol{x}_k)\bigr]^{-1}\nabla f(\boldsymbol{x}_k)
$$

!!! normal-comment "Stop criterion"
    $\Vert\nabla f(\boldsymbol{x}_k) > 0\Vert > \delta$ or in practice $\nabla f(\boldsymbol{x}_k)^T\bigl[\nabla^2 f(\boldsymbol{x}_k)\bigr]^{-1} > \delta$

#### Affine invariance

Do Newton's method on the two end of a affine transformation, you will get the same sequence (after transform).

Given $f$ and a invertible $\boldsymbol{A}$, let $g(\boldsymbol{y}) = f(\boldsymbol{Ay})$, 

$$
\nabla g(\boldsymbol{y}) = \boldsymbol{A}^{T} \nabla f(\boldsymbol{A} \boldsymbol{y}),
\quad
\nabla^{2} g(\boldsymbol{y}) = \boldsymbol{A}^{T} \nabla^{2} f(\boldsymbol{A} \boldsymbol{y}) \boldsymbol{A}
$$

If $\boldsymbol{x}_0 = \boldsymbol{Ay}_0$, then

$$
\begin{aligned}
\boldsymbol{y}_1
&= \boldsymbol{y}_0 - \bigl[\nabla^{2} g(\boldsymbol{y}_0)\bigr]^{-1} \nabla g(\boldsymbol{y}_0) \\
&= \boldsymbol{y}_0 - \bigl[\boldsymbol{A}^{T} \nabla^{2} f(\boldsymbol{x}_0) \boldsymbol{A}\bigr]^{-1} \boldsymbol{A}^{T} \nabla f(\boldsymbol{x}_0) \\
&= \boldsymbol{y}_0 - \boldsymbol{A}^{-1} \bigl[\nabla^{2} f(\boldsymbol{x}_0)\bigr]^{-1} \nabla f(\boldsymbol{x}_0) \\
&= \boldsymbol{A}^{-1} \bigl[\boldsymbol{x}_0 - \bigl[\nabla^{2} f(\boldsymbol{x}_0)\bigr]^{-1} \nabla f(\boldsymbol{x}_0)\bigr] \\
&= \boldsymbol{A}^{-1} \boldsymbol{x}_1
\end{aligned}
$$

!!! normal-comment "Connection to root finding"
    Finding the minimum is equal to find the root of its derivative!

![alt text](image-3.png)

It converges fast!

!!! normal-comment "Convergence Analysis: 1D case first"

    ![alt text](image-4.png)

    In general, Newton’s method does _not_ guarantee global convergence.  
    When it does converge, the convergence is usually very fast.

    If $f$ is $m$-strongly convex and $f''$ is $M$-Lipschitz, then 

    $$
    |x_{k + 1} - x^*| \leq \frac{M}{2m}|x_k - x^*|^2
    $$

    Let $\displaystyle\xi_k = \frac{M}{2m}|x_k - x^*|$, it converges iff. $\xi_0 < 1$, and it takes $O(\log \log \frac{1}{\varepsilon})$ to convergence!  

!!! remarks "Proof"

    $$
    \begin{aligned}
    \lvert x_{k+1} - x^{*} \rvert
    &= \lvert x_k - x^{*} - \bigl[f''(x_k)\bigr]^{-1} f'(x_k) \rvert \\[4pt]
    &= \lvert f''(x_k)\rvert^{-1} \cdot \lvert f'(x^{*}) - f'(x_k) - f''(x_k)(x^{*} - x_k) \rvert \qquad & f'(x^*) = 0 \\[4pt]
    &= \frac{\lvert x_k - x^{*} \rvert}{\lvert f''(x_k)\rvert}
    \cdot \left\lvert \int_{0}^{1} \bigl[f''(x_k + t(x^{*} - x_k)) - f''(x_k)\bigr] \,\mathrm{d}t \right\rvert \\[4pt]
    &\le \frac{\lvert x_k - x^{*} \rvert}{\lvert f''(x_k)\rvert}
    \cdot \int_{0}^{1} \left\lvert f''(x_k + t(x^{*} - x_k)) - f''(x_k) \right\rvert \mathrm{d}t \qquad & \text{Cauchy-Schwarz} \\[4pt]
    &\le \frac{\lvert x_k - x^{*} \rvert}{\lvert f''(x_k)\rvert}
    \cdot \int_{0}^{1} M t \lvert x_k - x^{*} \rvert \mathrm{d}t \qquad& M\text{-Lipschitz} \\[4pt]
    &= \frac{M}{2 \lvert f''(x_k)\rvert} \lvert x_k - x^{*} \rvert^{2} \\[4pt]
    &\le \frac{M}{2 m} \lvert x_k - x^{*} \rvert^{2} \qquad& m\text{-strong convexity}
    \end{aligned}
    $$