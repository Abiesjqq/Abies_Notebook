## 下降法（Descent Method）

### 基本梯度下降（Basic method）

$$
\boldsymbol{x}_{k + 1} = \boldsymbol{x}_k - t_k \nabla f(\boldsymbol{x}_k)
$$

其中 $t_k > 0$ 为步长。  
方向 $-\nabla f(\boldsymbol{x}_k)$ 是在点 $\boldsymbol{x}_k$ 处使函数值下降最快的方向：

!!! remarks "为何 $-\nabla f$ 是最速下降方向"

    若 $\lVert \boldsymbol{d}_k \rVert_2 = 1$，

    $$
    \lim_{t \to  0} 
    \frac{ f(\boldsymbol{x}_k) - f(\boldsymbol{x}_k + t\,\boldsymbol{d}_k) }{t}
    = -\, \boldsymbol{d}_k^{T} \nabla f(\boldsymbol{x}_k)
    \leq\lVert \nabla f(\boldsymbol{x}_k) \rVert_2,
    $$

    且当且仅当
    
    $$
    \boldsymbol{d}_k= - \frac{ \nabla f(\boldsymbol{x}_k) }{ \lVert \nabla f(\boldsymbol{x}_k) \rVert_2 }
    $$
    
    时取等号。

!!! remarks "Lipschitz 连续（Lipschitz Continuity）"
    若存在常数 $C > 0$，使得对任意 $\boldsymbol{x}, \boldsymbol{y}$ 都有

    $$
    \Vert \boldsymbol{f}(\boldsymbol{x}) - \boldsymbol{f}(\boldsymbol{y}) \Vert \leq C \Vert \boldsymbol{x} - \boldsymbol{y} \Vert,
    $$

    则称 $\boldsymbol{f}: \mathbb{R}^n \to \mathbb{R}^m$ 是 Lipschitz 连续的。

    !!! examples "例子"

        线性映射 $\boldsymbol{f}(\boldsymbol{x}) = \boldsymbol{Qx}$ 是 $\sigma_\text{max}(\boldsymbol{Q})$-Lipschitz 的。令 $\boldsymbol{d} = \boldsymbol{x} - \boldsymbol{y}$，则

        $$
        \Vert \boldsymbol{f}(\boldsymbol{x}) - \boldsymbol{f}(\boldsymbol{y}) \Vert^2 = \boldsymbol{d}^T\boldsymbol{Q}^T\boldsymbol{Q}\boldsymbol{d} \leq \lambda_\text{max}(\boldsymbol{Q}^T\boldsymbol{Q})\Vert \boldsymbol{d} \Vert^2.
        $$

!!! remarks "$L$-平滑性（$L$-smoothness）"
    若 $f$ 可导且梯度是 $L$-Lipschitz 的，即

    $$
    \Vert \nabla f(\boldsymbol{x}) - \nabla f(\boldsymbol{y}) \Vert \leq L \Vert \boldsymbol{x} - \boldsymbol{y} \Vert,
    $$

    则称 $f$ 是 $L$-smooth 的。

    !!! examples "例子"
        当 $\boldsymbol{Q} \succeq \boldsymbol{O}$ 时，函数 $f(\boldsymbol{x}) = \boldsymbol{x}^T\boldsymbol{Q}\boldsymbol{x}$ 是 $\sigma_\text{max}(\boldsymbol{Q})$-smooth。

    若 $f:\mathbb{R}^n \to \mathbb{R}$ 二阶连续可导且凸，则

    $$
    f \text{ 是 $L$-smooth}
    \Longleftrightarrow
    \nabla^2f(\boldsymbol{x}) \preceq L \boldsymbol{I}
    \Longleftrightarrow
    \lambda_\text{max}(\nabla^2f(\boldsymbol{x})) \leq L.
    $$

#### 二次上界（Quadratic upper bound）

若 $f$ 是 $L$-smooth 的，则对任意 $\boldsymbol{x}, \boldsymbol{y}$ 有

$$
f(\boldsymbol{y}) \leq f(\boldsymbol{x}) + \nabla f(\boldsymbol{x})^T(\boldsymbol{y} - \boldsymbol{x}) + \frac{L}{2}\Vert \boldsymbol{y} - \boldsymbol{x}\Vert^2.
$$

![二次上界示意](image.png)

!!! remarks "证明思路"

    先证明一维情形。令 $g(t)$ 是 $L_g$-smooth 的，即 $\lvert g'(t)-g'(s)\rvert\le L_g\lvert t-s\rvert$。

    $$
    \begin{aligned}
    g(1)
    &= g(0)+\int_{0}^{1} g'(t)\mathrm{d}t \\
    &= g(0)+g'(0)+\int_{0}^{1}\bigl[g'(t)-g'(0)\bigr]\mathrm{d}t \\
    &\leq g(0)+g'(0)+\int_{0}^{1}L_g t\mathrm{d}t
    \quad\text{因为 }\lvert g'(t)-g'(0)\rvert\leq L_g t \\[4pt]
    &= g(0)+g'(0)+\frac12 L_g.
    \end{aligned}
    $$

    对一般情形，取 $g(t)=f(\boldsymbol{x}+t\boldsymbol{d})$，其中 $\boldsymbol{d}=\boldsymbol{y}-\boldsymbol{x}$，并令 $L_g = L \lVert \boldsymbol{d}\rVert^{2}$。  
    检查 $g$ 的平滑性：

    $$
    \begin{aligned}
    \lvert g'(t)-g'(s)\rvert
    &= \bigl\lvert [\nabla f(\boldsymbol{x}+t\boldsymbol{d}) - \nabla f(\boldsymbol{x}+s\boldsymbol{d})]^T \boldsymbol{d} \bigr\rvert \\[2pt]
    &\le \lVert \nabla f(\boldsymbol{x}+t\boldsymbol{d}) - \nabla f(\boldsymbol{x}+s\boldsymbol{d}) \rVert \lVert \boldsymbol{d} \rVert 
    \quad\text{（Cauchy–Schwarz）} \\[2pt]
    &\le (t-s) L \lVert \boldsymbol{d}\rVert^{2} \quad (f \text{ 为 $L$-smooth}).
    \end{aligned}
    $$

**二次上界的推论**

对 $L$-smooth 的 $f$，梯度下降产生的序列 $\{\boldsymbol{x}_k\}$ 满足

$$
f(\boldsymbol{x}_{k + 1}) \leq f(\boldsymbol{x}_k) - t\left(1 - \frac{L}{2}t\right) \Vert \nabla f(\boldsymbol{x}_k) \Vert^2.
$$

!!! remarks "证明"
    直接把二次上界应用到 $\boldsymbol{x} = \boldsymbol{x}_k$、$\boldsymbol{y} = \boldsymbol{x}_{k+1}$，并利用
    $\nabla f(\boldsymbol{x}_k)^T(\boldsymbol{x}_{k + 1} - \boldsymbol{x}_k) = - t \Vert \nabla f(\boldsymbol{x}_k) \Vert^2$ 即可。

于是

$$
f(\boldsymbol{x}_{k}) - f(\boldsymbol{x}_{k + 1})  \geq  t\left(1 - \frac{L}{2}t\right) \Vert \nabla f(\boldsymbol{x}_k) \Vert^2 \geq \frac{1}{2}t \Vert \nabla f(\boldsymbol{x}_k) \Vert^2,
$$

只要 $\displaystyle 0 \leq t \leq \frac{1}{L}$（这就是常说的 **Descent Lemma**）。

**收敛性分析（Convergence Analysis）**

若步长 $t \in (0, 1 / L]$，且 $\boldsymbol{x}^*$ 为 $f$ 的最小点，则

$$
f(\boldsymbol{x}_k) - f(\boldsymbol{x}^*) \leq \frac{\Vert \boldsymbol{x}_0 - \boldsymbol{x}^*\Vert^2}{2tk}.
$$

!!! normal-comment "一些结论"
    - 当 $k \to \infty$ 时，$f(\boldsymbol{x}_k) \to f^*$；  
    - 任意收敛子序列的极限点都是最优解；  
    - 收敛速率为 $O(1/k)$，要达到 $f(\boldsymbol{x}_k) - f^* \leq \varepsilon$ 需要的迭代次数是 $O(1 / \varepsilon)$；  
    - 较大的步长一般能更快收敛，最佳固定步长形式理论上为 $t = 1 / L$，但 $L$ 通常未知。

!!! remarks "证明简要"

    $$
    \begin{aligned}
        \|\boldsymbol{x}_{k+1} - \boldsymbol{x}^*\|^2 
        - \|\boldsymbol{x}_k - \boldsymbol{x}^*\|^2
        &= \|\boldsymbol{x}_k - t\nabla f(\boldsymbol{x}_k) - \boldsymbol{x}^*\|^2
        - \|\boldsymbol{x}_k - \boldsymbol{x}^*\|^2 \\
        &= 2t \nabla f(\boldsymbol{x}_k)^T (\boldsymbol{x}^* - \boldsymbol{x}_k) 
        + t^2 \|\nabla f(\boldsymbol{x}_k)\|^2 \\
        & \leq 2t \bigl[f(\boldsymbol{x}^*) - f(\boldsymbol{x}_k)\bigr] + 2t\bigl[f(\boldsymbol{x}_k) - f(\boldsymbol{x}_{k+1})\bigr] \\
        &= 2t\bigl[f(\boldsymbol{x}^*) - f(\boldsymbol{x}_{k+1})\bigr].
    \end{aligned}
    $$

    对 $k = 0, \ldots, N-1$ 求和可得

    $$
    f(\boldsymbol{x}_N) - f(\boldsymbol{x}^*)
    \le \frac{\|\boldsymbol{x}_0 - \boldsymbol{x}^*\|^2}{2tN}.
    $$

### 梯度流（Gradient Flow）

梯度下降的连续时间版本为

$$
\frac{\mathrm{d}}{\mathrm{d}s}\tilde{\boldsymbol{x}}_s = -\nabla f(\tilde{\boldsymbol{x}}_s),
$$

其中 $s$ 类似“时间”。

若 $f$ 为凸函数，$\boldsymbol{x}^*$ 为其最小点，则在时间 $T$ 时有

$$
f(\tilde{\boldsymbol{x}}_T) - f(\boldsymbol{x}^*) 
\le \frac{\|\tilde{\boldsymbol{x}}_0 - \boldsymbol{x}^*\|^2}{2T}.
$$

!!! remarks "证明思路"

    $$
    \begin{aligned}
        \frac{\mathrm d}{\mathrm ds}\|\tilde{\boldsymbol{x}}_s - \boldsymbol{x}^*\|^2
        &= 2\langle \tilde{\boldsymbol{x}}_s - \boldsymbol{x}^*, 
        \frac{\mathrm d}{\mathrm ds}\tilde{\boldsymbol{x}}_s\rangle
        = 2\langle \nabla f(\tilde{\boldsymbol{x}}_s), \boldsymbol{x}^* - \tilde{\boldsymbol{x}}_s\rangle \\[2pt]
        &\leq 2\bigl(f(\boldsymbol{x}^*) - f(\tilde{\boldsymbol{x}}_s)\bigr).
    \end{aligned}
    $$

    在 $[0, T]$ 上积分，并利用 $f(\tilde{\boldsymbol{x}}_s) \ge f(\tilde{\boldsymbol{x}}_T)$ 即可得到所需不等式。

### 强凸性（Strong Convexity）

若对某个 $m > 0$，函数

$$
\tilde{f}(\boldsymbol{x}) = f(\boldsymbol{x}) - \frac{m}{2}\Vert \boldsymbol{x}\Vert^2
$$

是凸的，则称 $f$ 是 **$m$-strongly convex**。

#### 强凸函数的一阶刻画

可导函数 $f$ 是 $m$-strongly convex 当且仅当对任意 $\boldsymbol{x}, \boldsymbol{y}$ 有

$$
f(\boldsymbol{y})\geq f(\boldsymbol{x}) + \nabla f(\boldsymbol{x})^T(\boldsymbol{y} - \boldsymbol{x}) + \frac{m}{2}\Vert \boldsymbol{x} - \boldsymbol{y}\Vert^2.
$$

!!! remarks "证明"

    从 $\tilde{f}$ 的凸性展开即可得到上述不等式，反之亦同。

#### 强凸函数的二阶刻画

若 $f$ 二阶可导，则 $f$ 是 $m$-strongly convex 当且仅当

$$
\nabla^2 f(\boldsymbol{x}) \succeq m\boldsymbol{I}
\Longleftrightarrow
\lambda_{\text{min}}(\nabla^2 f(\boldsymbol{x})) \geq m.
$$

!!! remarks "强凸 + 平滑带来的界"

    若 $f$ 既是 $m$-strongly convex 又是 $L$-smooth，则对任意 $\boldsymbol{x}, \boldsymbol{y}$ 有

    $$
    \frac{m}{2}\,\|\boldsymbol{x}-\boldsymbol{y}\|^{2}\leq
    f(\boldsymbol{y})-f(\boldsymbol{x})-\nabla f(\boldsymbol{x})^{T}(\boldsymbol{y}-\boldsymbol{x})\leq
    \frac{L}{2}\,\|\boldsymbol{x}-\boldsymbol{y}\|^{2},
    $$

    更精确地，可以写成

    $$
    \frac{1}{2}\lambda_{\text{min}}(\nabla^2 f(\boldsymbol{x}))\,\|\boldsymbol{x}-\boldsymbol{y}\|^{2}
    \leq
    f(\boldsymbol{y})-f(\boldsymbol{x})-\nabla f(\boldsymbol{x})^{T}(\boldsymbol{y}-\boldsymbol{x})
    \leq 
    \frac{1}{2}\lambda_{\text{max}}(\nabla^2 f(\boldsymbol{x}))\,\|\boldsymbol{x}-\boldsymbol{y}\|^{2}.
    $$

若 $f$ 是 $m$-strongly convex，则对任意 $\boldsymbol{x}$ 与最优点 $\boldsymbol{x}^*$ 有

$$
f(\boldsymbol{x}) - f(\boldsymbol{x}^*) \leq \frac{1}{2m}\Vert \nabla f(\boldsymbol{x}) \Vert^2.
$$

!!! remarks "证明"

    $$
    \begin{aligned}
        f(\boldsymbol{x}^*) &= \min_{\boldsymbol{y}}f(\boldsymbol{y}) \\
        &\geq \min_{\boldsymbol{y}} \left[f(\boldsymbol{x}) + \nabla f(\boldsymbol{x})^T(\boldsymbol{y} - \boldsymbol{x}) + \frac{m}{2}\Vert \boldsymbol{x} - \boldsymbol{y}\Vert^2\right] \\
        &\geq f(\boldsymbol{x}) - \frac{1}{2m}\Vert \nabla f(\boldsymbol{x})\Vert^2,
    \end{aligned}
    $$

    其中下界在选择 $\displaystyle \boldsymbol{y} = \boldsymbol{x} - \frac{1}{m}\nabla f(\boldsymbol{x})$ 时达到。

#### 强凸情形下梯度流的收敛

若 $f$ 是 $m$-strongly convex，则在梯度流下有

$$
\frac{\mathrm{d}}{\mathrm{d}s}\,\|\boldsymbol{\tilde{x}}_{s}-\boldsymbol{x}^*\|^{2}
\le -m\|\boldsymbol{\tilde{x}}_{s}-\boldsymbol{x}^*\|^{2},
$$

从而

$$
\Vert \tilde{\boldsymbol{x}}_T - \boldsymbol{x}^*\Vert^2 \leq \mathrm{e}^{-mT}\Vert \tilde{\boldsymbol{x}}_0 - \boldsymbol{x}^*\Vert^2.
$$

!!! normal-comment "用到的微分不等式"

    若 $\displaystyle \frac{\mathrm{d}}{\mathrm{d}x}f(x) \leq C f(x)$，则

    $$
    [\mathrm{e}^{Cx} f(x)]' \leq 0
    \Longrightarrow
    \mathrm{e}^{Cx}f(x) \leq f(0)
    \Longrightarrow
    f(x) \leq \mathrm{e}^{-Cx}f(0).
    $$

**定理**：若 $f$ $m$-strongly convex 且 $L$-smooth，$\boldsymbol{x}^*$ 为其最小点，则当步长 $\displaystyle t \in (0, 1/L]$ 时，梯度下降产生的序列 $\{\boldsymbol{x}_k\}$ 满足

$$
f(\boldsymbol{x}_k)-f(\boldsymbol{x}^*)
\le (1-mt)^{k}\bigl[f(\boldsymbol{x}_0)-f(\boldsymbol{x}^*)\bigr],
$$

$$
\|\boldsymbol{x}_k-\boldsymbol{x}^*\|^{2}
\le (1-mt)^{k}\|\boldsymbol{x}_0-\boldsymbol{x}^*\|^{2}.
$$

!!! normal-comment "补充说明"

    - 为了达到 $f(\boldsymbol{x}_k) - f(\boldsymbol{x}^*) \leq \varepsilon$，所需迭代次数为 $O(\log (1 / \varepsilon))$，与精度位数线性相关；  
    - 并且有

    $$
    \frac{m}{2}\,\|\boldsymbol{x}_k-\boldsymbol{x}^*\|^{2}\leq
    f(\boldsymbol{x}_k)-f(\boldsymbol{x}^*)\leq
    \frac{L}{2}\,\|\boldsymbol{x}_k-\boldsymbol{x}^*\|^{2}.
    $$

!!! remarks "证明思路"

    类似前面的推导，

    $$
    \begin{aligned}
        \|\boldsymbol{x}_{k+1} - \boldsymbol{x}^*\|^2 
        - \|\boldsymbol{x}_k - \boldsymbol{x}^*\|^2
        &= 2t \nabla f(\boldsymbol{x}_k)^T (\boldsymbol{x}^* - \boldsymbol{x}_k) 
        + t^2 \|\nabla f(\boldsymbol{x}_k)\|^2.
    \end{aligned}
    $$

    由强凸性有

    $$
    \nabla f(\boldsymbol{x}_k)^{T}(\boldsymbol{x}^*-\boldsymbol{x}_k)
    \le f(\boldsymbol{x}^*) - f(\boldsymbol{x}_k)
    - \frac{m}{2}\|\boldsymbol{x}_k-\boldsymbol{x}^*\|^{2},
    $$

    结合 $L$-smooth 得到递推不等式，从而推得上述线性收敛结论。

## 条件数与改进方法（Condition number and Enhanced methods）

对二次函数 $\displaystyle f(\boldsymbol{x}) = \frac{1}{2}\boldsymbol{x}^T\boldsymbol{Qx}$，可以用一个数来刻画收敛“好不好”：**条件数（condition number）**。

### 条件数（Condition number）

对满足 $\boldsymbol{Q} \succeq \boldsymbol{O}$ 的矩阵 $\boldsymbol{Q} \in \mathbb{R}^{n \times n}$，定义

$$
\kappa(\boldsymbol{Q}) = \frac{\lambda_{\text{max}}(\boldsymbol{Q})}{\lambda_{\text{min}}(\boldsymbol{Q})}.
$$

它刻画了 $\displaystyle f(\boldsymbol{x}) = \frac{1}{2}\boldsymbol{x}^T\boldsymbol{Qx}$ 的等高线被“拉伸”的程度。  
在非二次情形下，$\kappa(\nabla^2 f(\boldsymbol{x}))$ 起到类似的作用。

当 $\kappa(\boldsymbol{Q})$ 较小时，问题 $\displaystyle \min_{\boldsymbol{x}}\frac{1}{2}\boldsymbol{x}^T\boldsymbol{Qx}$ 称为 **well conditioned**；反之则称为 **ill conditioned**。

### 精确线搜索（Exact line search）

在每一步中令

$$
t_k = \arg\min_{s > 0} f(\boldsymbol{x}_k - s\nabla f(\boldsymbol{x}_k)),
$$

即在梯度方向上做一维最优化。  
然而在实际中精确线搜索往往代价较高。

!!! examples "示意"

    ![精确线搜索示意图](image-1.png)

    即使每步都做 exact line search，也不一定保证整体收敛速度理想。

!!! remarks "收敛性分析（结论）"

    对 $m$-strongly convex 且 $L$-smooth 的 $f$，精确线搜索下有

    $$
    f(\boldsymbol{x}_k)-f(\boldsymbol{x}^*)
    \le \left(1-\frac{m}{L}\right)^{k}
    \bigl[f(\boldsymbol{x}_0)-f(\boldsymbol{x}^*)\bigr],
    $$

    证明思路与前述线性收敛分析类似，这里省略。

### 回溯线搜索（Backtracking line search）

回溯线搜索通过逐步缩小步长，在保证下降的前提下自动选择合适的 $t$。  
取常数 $\alpha, \beta \in (0, 1)$，在每一步：

从某个初始步长 $t$ 开始；  
令

$$
t \gets \beta t
$$

反复缩小步长，直到满足

$$
f(\boldsymbol{x} - t\nabla f(\boldsymbol{x})) \leq f(\boldsymbol{x}) - \alpha t \Vert \nabla f(\boldsymbol{x})\Vert^2,
$$

这就是著名的 **Armijo’s rule**。

