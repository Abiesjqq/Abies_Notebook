
Can we apply gradient descent to a constrained problem? Sometimes it will be infeasible. So we gonna project it onto the feasible set $\Omega$!

## Projected gradient descent

The gradient descent step:

$$
\boldsymbol{x}_{k + 1} = \boldsymbol{x}_k - t\boldsymbol{g}(\boldsymbol{x}_k), \quad \boldsymbol{g}(\boldsymbol{x}) = \frac{1}{t}\left(\boldsymbol{x} - \mathcal{P}_\Omega (\boldsymbol{x} - t\nabla f(\boldsymbol{x}))\right)
$$

i.e. $\displaystyle \boldsymbol{x}_{k + 1} = \mathcal{P}_\Omega (\boldsymbol{x}_k - t\nabla f(\boldsymbol{x}_k))$

![alt text](image.png){ style="width:50%" }

For convex $f$ and $\Omega$, $\boldsymbol{g}(\boldsymbol{x}^*) = \boldsymbol{0}$ iff. $\boldsymbol{x}^*$ is a minimum of $f$ over $\Omega$.

!!! remarks "Proof"

    $$
    \begin{aligned}
        \boldsymbol{g}(\boldsymbol{x}^*) = \boldsymbol{0} & \Longleftrightarrow \boldsymbol{x}^* =  \mathcal{P}_\Omega (\boldsymbol{x}^* - t\nabla f(\boldsymbol{x}^*)) \\
        & \Longleftrightarrow \langle \boldsymbol{x}^* - t\nabla f(\boldsymbol{x}^*) - \boldsymbol{x}^*, \boldsymbol{z} - \boldsymbol{x}^*\rangle \leq 0, \quad \forall \boldsymbol{z} \in \Omega \\
        & \Longleftrightarrow \langle \nabla f(\boldsymbol{x}^*), \boldsymbol{z} - \boldsymbol{x}^* \rangle \geq 0, \quad \forall\boldsymbol{z} \in \Omega \\
        & \Longleftrightarrow \boldsymbol{x}^* \text{ is the minimum}.
    \end{aligned}
    $$

### Some projections

#### Box constraint

$\Omega = \{\boldsymbol{x}\ |\ \boldsymbol{a} \leq \boldsymbol{x} \leq \boldsymbol{b}\}$, the projection:

$$
\begin{aligned}
    \min_{\boldsymbol{x}}\ &\frac{1}{2}\Vert \boldsymbol{x} - \boldsymbol{y}\Vert^2 \\
    \text{s.t.}\ & \boldsymbol{a} \leq \boldsymbol{x} \leq \boldsymbol{b}
\end{aligned}
$$

Easily $\displaystyle \mathcal{P}_\Omega(\boldsymbol{x}) = \min\{\boldsymbol{b}, \max\{\boldsymbol{a}, \boldsymbol{y}\}\}$

#### Affine constraint

$$
\begin{aligned}
    \min_{\boldsymbol{x}}\ &\frac{1}{2}\Vert \boldsymbol{x} - \boldsymbol{y}\Vert^2 \\
    \text{s.t.}\ & \boldsymbol{Ax} = \boldsymbol{b}
\end{aligned}
$$

By Lagrange condition, there exists $\boldsymbol{\lambda}$, 

$$
\boldsymbol{x} - \boldsymbol{y} + \boldsymbol{A}^T\boldsymbol{\lambda} = \boldsymbol{0} \Longleftrightarrow \boldsymbol{x} = \boldsymbol{y} - \boldsymbol{A}^T\boldsymbol{\lambda}
$$

plugin and solve, we can get

$$
\mathcal{P}_{\Omega}(\boldsymbol{y}) = \boldsymbol{y} - \boldsymbol{A}^T(\boldsymbol{AA}^T)^{-1}(\boldsymbol{Ay} - \boldsymbol{b})
$$

#### $l_2$ Ball

$$
\begin{aligned}
    \min_{\boldsymbol{x}}\ &\frac{1}{2}\Vert \boldsymbol{x} - \boldsymbol{y}\Vert^2 \\
    \text{s.t.}\ & \Vert \boldsymbol{x} \Vert_2^2 \leq r^2
\end{aligned}
$$

By KKT condition, there exists $\mu \geq 0$, 

$$
\boldsymbol{x} - \boldsymbol{y} + 2\mu \boldsymbol{x} = 0 \Longleftrightarrow \boldsymbol{x} = \frac{1}{1 + 2\mu}\boldsymbol{y}
$$

we can get

$$
\mathcal{P}_{\Omega}(\boldsymbol{y}) = \min \left\{1, \frac{r}{\Vert \boldsymbol{y}\Vert_2}\boldsymbol{y}\right\}
$$

#### Ellipsoids

$\Omega = \{\boldsymbol{x}\ |\ \boldsymbol{x}^T\boldsymbol{Qx} \leq 1\}$ where $\boldsymbol{Q} \succ \boldsymbol{O}$

$$
\begin{aligned}
    \min_{\boldsymbol{x}}\ &\frac{1}{2}\Vert \boldsymbol{x} - \boldsymbol{y}\Vert^2 \\
    \text{s.t.}\ & \boldsymbol{x}^T\boldsymbol{Qx} \leq 1
\end{aligned}
$$

By KKT conditions, there exists $\mu \geq 0$, 

$$
\boldsymbol{x} - \boldsymbol{y} + 2\mu \boldsymbol{Qx} = 0 \Longleftrightarrow \boldsymbol{x} = (\boldsymbol{I} + 2\mu\boldsymbol{Q})^{-1} \boldsymbol{y}
$$

If $\boldsymbol{y} \in \Omega$, then $\mu = 0, \boldsymbol{x} = \boldsymbol{y}$.  
If $\boldsymbol{y} \notin \Omega$, then $\mu > 0$. Let $\boldsymbol{Q} = \boldsymbol{U\Lambda U}^T$ and $\boldsymbol{z} = \boldsymbol{U}^T\boldsymbol{y}$, we have the equation of $\mu$:

$$
1 = \boldsymbol{x}^T\boldsymbol{Qx} = \boldsymbol{y}^T(\boldsymbol{I} + 2\mu\boldsymbol{Q})^{-1}\boldsymbol{Q}(\boldsymbol{I} + 2\mu\boldsymbol{Q})^{-1}\boldsymbol{y} = \sum_{i = 1}^n \frac{\lambda_i z_i^2}{(1 + 2\mu \lambda_i)^2}
$$

#### $l_1$ Ball

To be completed...

### Convergence analysis

Let $\Omega$ be a nonempty convex set and $f$ an $L$-smooth convex function. $\boldsymbol{x}^*$ is the minimum of $f$ over $\Omega$. The sequence produced by PGD with step size $t \leq \dfrac{1}{L}$ satisfies $f(\boldsymbol{x}_{k + 1}) \leq f(\boldsymbol{x}_k)$ and

$$
f(\boldsymbol{x}_k) - f(\boldsymbol{x}^*) \leq \frac{\Vert \boldsymbol{x}^* - \boldsymbol{x}_0\Vert^2}{2tk}
$$

!!! remarks "Proof"

    Let indicator $I_\Omega$ by

    $$
    I_\Omega(\boldsymbol{x}) = \begin{cases}
        0, & \quad \boldsymbol{x} \in \Omega \\
        +\infty, & \quad \boldsymbol{x} \notin \Omega
    \end{cases}
    $$

    Then 

    $$
    \operatorname{prox}_{I_\Omega}(\boldsymbol{y}) = \arg \min_{\boldsymbol{x}}\frac{1}{2}\Vert \boldsymbol{x} - \boldsymbol{y} \Vert^2 + I_\Omega (\boldsymbol{x}) = \arg \min_{\boldsymbol{x} \in \Omega}\frac{1}{2}\Vert \boldsymbol{x} - \boldsymbol{y} \Vert^2 = \mathcal{P}_{\Omega}(\boldsymbol{y})
    $$

    Then

    $$
    \boldsymbol{x}_{k + 1} = \operatorname{prox}_{t_kI_\Omega}(\boldsymbol{x_k} - t_k\nabla f(\boldsymbol{x}_k))
    $$

    Then it is the same as **proximal gradient descent**!

## Newton's Method

In a QP:

$$
\begin{aligned}
    \min_x\  &\frac{1}{2}\boldsymbol{x}^T\boldsymbol{Q}\boldsymbol{x} + \boldsymbol{c}^T\boldsymbol{x} \\
    \text{s.t.}\ 
    \ &\boldsymbol{Ax} = \boldsymbol{b}
\end{aligned}
$$

the Lagrange condition can be written as

$$
\begin{pmatrix}
    \boldsymbol{Q} & \boldsymbol{A}^T \\
    \boldsymbol{A} & \boldsymbol{O}
\end{pmatrix} \begin{pmatrix}
    \boldsymbol{x}^* \\ \boldsymbol{\lambda}^*
\end{pmatrix} = \begin{pmatrix}
    -\boldsymbol{g} \\ \boldsymbol{b}
\end{pmatrix}
$$

The coefficient matrix $\boldsymbol{K} = \begin{pmatrix}\boldsymbol{Q} & \boldsymbol{A}^T \\\boldsymbol{A} & \boldsymbol{O}\end{pmatrix}$ is called the KKT matrix.

!!! normal-comment "Nonsingularity of KKT matrix"

The following conditions are equivalent:

$\boldsymbol{K}$ is nonsingular  
QP has a unique minimum  
$\boldsymbol{F}^T\boldsymbol{QF} \succ \boldsymbol{O}$ for any $\boldsymbol{F} \in \mathbb{R}^{n \times (n - k)}$ s.t. $\operatorname{Range}(\boldsymbol{F}) = \operatorname{Null}(\boldsymbol{A})$, i.e. the columns of $\boldsymbol{F}$ form a basis of $\operatorname{Null}(\boldsymbol{A})$  
$\boldsymbol{Av} = \boldsymbol{0}, \boldsymbol{v} \neq \boldsymbol{0} \Longrightarrow \boldsymbol{v}^T\boldsymbol{Qv} > 0$  
$\operatorname{Null}(\boldsymbol{Q}) \cap \operatorname{Null}(\boldsymbol{A}) = \{\boldsymbol{0}\}$

!!! remarks "Proof"

Hence $\boldsymbol{A}$ is full rank, each $\boldsymbol{x}^*$ corresponds to a unique $\boldsymbol{\lambda}^*$ satisfying the Lagrange condition.

To be completed...

### Newton’s method with feasible start

Consider the convex problem with equality constraints,

$$
\begin{aligned}
    \min_x\  &f(\boldsymbol{x}) \\
    \text{s.t.}\ 
    \ &\boldsymbol{Ax} = \boldsymbol{b}
\end{aligned}
$$

The Newton direction at $\boldsymbol{x}_k$ is the solution to the approximation

$$
\begin{aligned}
    \min_x\  &\hat{f}(\boldsymbol{x}_k + \boldsymbol{v}) = f(\boldsymbol{x}_k) + \nabla f(\boldsymbol{x}_k)^T\boldsymbol{v} + \frac{1}{2}\boldsymbol{v}^T\nabla^2 f(\boldsymbol{x}_k)\boldsymbol{v} \\
    \text{s.t.}\ 
    \ &\boldsymbol{A}(\boldsymbol{x_k} + \boldsymbol{v}) = \boldsymbol{b}
\end{aligned}
$$

i.e.

$$
\begin{pmatrix}
    \nabla^2 f(\boldsymbol{x}_k) & \boldsymbol{A}^T \\ \boldsymbol{A} & \boldsymbol{O}
\end{pmatrix}\begin{pmatrix}
    \boldsymbol{v}\\ \boldsymbol{\lambda}
\end{pmatrix} = \begin{pmatrix}
        -\nabla f(\boldsymbol{x}_k) \\ \boldsymbol{0}
\end{pmatrix}
$$

!!! remarks "Lamma"

    By the KKT condition, we have a critical equation

    $$
    \nabla f(\boldsymbol{x}_k) + \boldsymbol{A}^T \boldsymbol{\lambda} = -\nabla^2 f(\boldsymbol{x}_k)\boldsymbol{v}
    $$

    $$
    \begin{aligned}
        \boldsymbol{x} \text{ is the minimal}& \Longleftrightarrow \nabla^2 f(\boldsymbol{x}_k)\boldsymbol{v} = 0 \\
        & \Longleftrightarrow \boldsymbol{v} \in \operatorname{Null}(\boldsymbol{A}) \cap \operatorname{Null}(\nabla^2 f(\boldsymbol{x}_k)) = \{\boldsymbol{0}\} \\
        &\Longleftrightarrow \boldsymbol{v} = \boldsymbol{0}
    \end{aligned}
    $$

$$
\begin{aligned}
&\textbf{Initialization:}\ \boldsymbol{x} \leftarrow \boldsymbol{x}_0 \in \Omega
\ (\text{i.e. } A\boldsymbol{x}_0=\boldsymbol{b})\\
&\textbf{Repeat}\\
&\quad \text{Compute Newton direction } \boldsymbol{v} \text{ by solving}\\
&\quad
\begin{pmatrix}
\nabla^2 f(\boldsymbol{x}) & A^T\\
A & 0
\end{pmatrix}
\begin{pmatrix}
\boldsymbol{v}\\
\boldsymbol{\lambda}
\end{pmatrix}
=
\begin{pmatrix}
-\nabla f(\boldsymbol{x})\\
\boldsymbol{0}
\end{pmatrix}\\
&\quad t \leftarrow 1\\
&\quad \textbf{while } 
f(\boldsymbol{x}+t\boldsymbol{v})
>
f(\boldsymbol{x})+\alpha t\nabla f(\boldsymbol{x})^T\boldsymbol{v}
\ \textbf{do}\\
&\qquad t \leftarrow \beta t\\
&\quad \textbf{end while}\\
&\quad \boldsymbol{x} \leftarrow \boldsymbol{x}+t\boldsymbol{v}\\
&\textbf{Until } \|\boldsymbol{v}\|\le\delta\\
&\textbf{Return } \boldsymbol{x}
\end{aligned}
$$

### Convergence analysis

Let $\boldsymbol{F} \in \mathbb{R}^{n \times (n - k)}$ be a matrix whose columns are linearly independent solutions to $\boldsymbol{Av} = \boldsymbol{0}$, then fix a feasible $\tilde{\boldsymbol{x}}$, every $\boldsymbol{x}$ has a unique representation of the form $\boldsymbol{x} = \tilde{\boldsymbol{x}} + \boldsymbol{Fz}$:

$$
\Omega = \{\boldsymbol{x}\ |\ \boldsymbol{Ax} = \boldsymbol{b}\} = \{\tilde{\boldsymbol{x}} + \boldsymbol{Fz}\ |\ \boldsymbol{z} \in \mathbb{R}^{n - k}\}
$$

and the former problem reduces to unconstrained problem by $\boldsymbol{x} = \tilde{\boldsymbol{x}}_0 + \boldsymbol{Fz}$:

$$
\begin{cases}
    \min_{\boldsymbol{x}} f(\boldsymbol{x}) \\
    \boldsymbol{Ax} = \boldsymbol{b}
\end{cases} \Longleftrightarrow \min_{\boldsymbol{z}} g(\boldsymbol{z}) = f(\tilde{\boldsymbol{x}} + \boldsymbol{Fz})
$$

and accordingly $\boldsymbol{x}_k = \tilde{\boldsymbol{x}} + \boldsymbol{Fz}_k$

!!! remarks "Proof"

    We only need to show $\boldsymbol{x}_1 = \tilde{\boldsymbol{x}} + \boldsymbol{Fz}_1$ and reduction

    Noting that $\boldsymbol{v}$ at $\boldsymbol{x}_0$ is the unique solution to

    $$
    \begin{aligned}
        \min_{\boldsymbol{v}_0}\ & \hat{f}(\boldsymbol{x}_0 + \boldsymbol{v}) = f(\boldsymbol{x}_0) + \nabla f(\boldsymbol{x})^T\boldsymbol{v} + \frac{1}{2}\boldsymbol{v}^T\nabla^2 f(\boldsymbol{x}_0) \boldsymbol{v} \\
        &\boldsymbol{Av} = \boldsymbol{0}
    \end{aligned}
    $$

    and $\boldsymbol{u}$ is the unique solution of

    $$
    \begin{aligned}
        \min_{\boldsymbol{v}_0}\ g(\boldsymbol{z}_0 + \boldsymbol{u}) &= g(\boldsymbol{z}_0) + \nabla g(\boldsymbol{z})^T\boldsymbol{u} + \frac{1}{2}\boldsymbol{u}^T\nabla^2 g(\boldsymbol{z}_0) \boldsymbol{u} \\
        &= f(\boldsymbol{x}_0) + \nabla f(\boldsymbol{x})^T\boldsymbol{Fu} + \frac{1}{2}\boldsymbol{u}^T\boldsymbol{F}^T\nabla^2 f(\boldsymbol{x}_0) \boldsymbol{Fu}
    \end{aligned}
    $$

    so $\boldsymbol{v} = \boldsymbol{Fu}$

    So in the backtracking line search, the conditions $t_0$ in $f$ and $g$ are the same! Then

    $$
    \boldsymbol{x}_1 = \boldsymbol{x}_0 + t_0 \boldsymbol{v}_0 = \tilde{\boldsymbol{x}} + \boldsymbol{F}(\boldsymbol{z}_0 + t_0 \boldsymbol{u}_0) = \tilde{\boldsymbol{x}} + \boldsymbol{Fz}_1
    $$

### Newton's method with infeasible start

To be completed...

## Barrier method

In some cases of ICP:

$$
\begin{aligned}
    &\min_{\boldsymbol{x}}f(\boldsymbol{x}) \qquad (\text{ICP})\\
    &\ \text{s.t.} \ \  \boldsymbol{Ax} = \boldsymbol{b} \\
    & \quad \quad g_j(\boldsymbol{x}) \leq 0, \ j = 1, \ldots, m 
\end{aligned}
$$

where $f, g_j$ are smooth convex functions.

So far, we can solve some special cases  
- When projection onto the feasible set is easy, we can use
projected gradient descent  
- When there are no inequality constraints, we can use Newton’s
method or projected gradient descent  

As before, we can reform ICP to 

$$
\begin{aligned}
    &\min_{\boldsymbol{x}}f(\boldsymbol{x}) + I_{\bar{S}}(\boldsymbol{x})\\
    &\ \text{s.t.} \ \  \boldsymbol{Ax} = \boldsymbol{b} \\
\end{aligned}
$$

where $\bar{S} = \{\boldsymbol{x}\ |\ g_j(\boldsymbol{x}) \leq 0,\ j = 1, \ldots, m\}$

However, $I(\boldsymbol{x})$ is not smooth. So we gonna approximate $I(\boldsymbol{x})$ by so-called **barrier functions**.

### Barrier function

A continuous function $B: S \to \mathbb{R}$ is a barrier function for $S$ if $B(\boldsymbol{x})$ as $\boldsymbol{x} \in S$ approaches the boundary of $S$.

!!! normal-comment "Common barrier functions"

    **logarithmic barrier function**

    $$
    B(\boldsymbol{x}) = -\sum_{j = 1}^m \log (-g_j(\boldsymbol{x}))
    $$

    gradient

    $$
    \nabla B(\boldsymbol{x}) = \sum_{j = 1}^m \frac{1}{g_j(-\boldsymbol{x})}\nabla g_j(\boldsymbol{x})
    $$

    Hessian 

    $$
    \nabla^2 B(\boldsymbol{x}) = \sum_{j = 1}^m \frac{1}{g_j^2(-\boldsymbol{x})}\nabla g_j(\boldsymbol{x})g_j(\boldsymbol{x})^T + \sum_{j = 1}^m\frac{1}{g_j(-\boldsymbol{x})}\nabla^2 g_j(\boldsymbol{x})
    $$

    **inverse barrier function**

    $$
    B(\boldsymbol{x}) = -\sum_{j = 1}^m\frac{1}{g_j(\boldsymbol{x})}
    $$

Then we can approximate $I_{\bar{S}}(\boldsymbol{x})$ by $\dfrac{1}{\tau}B(\boldsymbol{x})$

$$
\begin{aligned}
    &\min_{\boldsymbol{x}}F(\boldsymbol{x}) = f(\boldsymbol{x}) + \frac{1}{\tau}B(\boldsymbol{x})\\
    &\ \text{s.t.} \ \  \boldsymbol{Ax} = \boldsymbol{b} \\
\end{aligned}
$$

!!! normal-comment "Note"

For it to be feasible, ICP should be strictly feasible, i.e. $\{x \ |\ \boldsymbol{Ax} = \boldsymbol{b}, g_j(\boldsymbol{x}) < 0\}$ is nonempty.

$\operatorname{dom} f \neq \mathbb{R}^n$, so always check whether $\boldsymbol{x}_k + t\boldsymbol{d}_k \in \operatorname{dom} F$.

### Central path

The solution $\boldsymbol{x}^*(\tau)$ to it is called a central point, as $\tau > 0$ varies, the curve $\boldsymbol{x}^*(\tau)$ is called a central path if ICP

#### Suboptimality gap of central point

$\boldsymbol{x}^*(\tau)$ is $\dfrac{m}{\tau}$-suboptimal to ICP, i.e.

$$
f(\boldsymbol{x}^*(\tau)) - f^* \leq \frac{m}{\tau}
$$

!!! remarks "Proof"

    The lagrangian is 

    $$
    \mathcal{L}(\boldsymbol{x}, \boldsymbol{\lambda}, \boldsymbol{\mu}) = f(\boldsymbol{x}) + \sum_{j = 1}^m \underbrace{\mu_j}_{\geq 0}\underbrace{ g_j(\boldsymbol{x})}_{\leq 0} + \underbrace{(\boldsymbol{Ax} - \boldsymbol{b})}_{=0} \lambda \leq f(\boldsymbol{x})
    $$

    By the lagrange condition, there exists $\boldsymbol{\lambda}^*, \boldsymbol{\mu}^*$,

    $$
    \nabla_{\boldsymbol{x}} \mathcal{L}(\boldsymbol{x}^*(\tau), \boldsymbol{\lambda}^*, \boldsymbol{\mu}^*) = \nabla f(\boldsymbol{x}^*) + \sum_{j = 1}^m \mu_j^* \nabla g(\boldsymbol{x}^*(\tau)) + \boldsymbol{A}^T\boldsymbol{\lambda}^* = \boldsymbol{0}
    $$

    where $\mu_j^* = -\dfrac{1}{\tau g_j(\boldsymbol{x}^*(\tau))} > 0$

    Noting that $\boldsymbol{x}^*(\tau)$ minimizes $\mathcal{L}(\boldsymbol{x}, \boldsymbol{\lambda}^*, \boldsymbol{\mu}^*)$, we have

    $$
    \begin{aligned}
        f^* &= \inf_{\boldsymbol{x} \in X}f(\boldsymbol{x}) \geq \inf_{\boldsymbol{x} \in X} \mathcal{L}(\boldsymbol{x}, \boldsymbol{\lambda}^*, \boldsymbol{\mu}^*) \\
        &\geq \inf_{\boldsymbol{x} \in X} \mathcal{L}(\boldsymbol{x}^*(\tau), \boldsymbol{\lambda}^*, \boldsymbol{\mu}^*) \\
        &= f(\boldsymbol{x}^*(\tau)) + \sum_{j = 1}^m\underbrace{\mu_j^* g_j(\boldsymbol{x}^*(\tau))}_{=-\frac{1}{\tau} \text{ by def.}} + \underbrace{(\boldsymbol{Ax}^*(\tau) - \boldsymbol{b})}_{=0}\boldsymbol{\lambda}^* \\
        &= f(\boldsymbol{x}^*) - \frac{m}{\tau}
    \end{aligned}
    $$

We call it the modified KKT conditions