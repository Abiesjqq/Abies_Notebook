## CPU vs GPU

Differences:

- **CPU**: fewer cores, but each core is much faster and much more capable; has small cache while the majority of memory is in RAM; great at sequaltial tasks.
- **GPU**: more cores, but each core is much slower and "dumber"; has its own RAM built and cacheing system; great for parallel tasks (matrix multiplication, convolution, etc.).

Programming GPU: CUDA (cuDNN and other optimiazed APIs), OpenCL.

If you aren't careful, training can bottleneck on reading data and transfering to GPU! Solution: read all data from RAM; use SSD instead of HDD; use multiple CPU threads to prefetch data.

## Deep Learning Frameworks

One goal of deep learning frameworks: enable vectors to calculate on GPU, automatically do back propagation and gradient process...

### Tensorflow

Framework of Tensorflow:

```py
initialize N, D, H
x/y/w1/w2 = tf.placeholder(tf.float32, shape)

set h as x * w1
set y_pred as h * w2
set loss as Euclidien distance

grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])

with tf.Session() as sess:
    set values as {x, w1, w2, y}
    initialize learing_rate
    for t in range(50):
        out = sess.run([loss, grad_w1, grad_w2], feed_dict = values)
        loss_val, grad_w1_val, grad_w2_val = out
        value[w1] -= learning_rate * grad_w1_val;
        value{w2} -= learning_rate * grad_w2_val;
```

Change w1 and w2 from `placeholder` to `Variable` (values live inside the graph) to avoid GPU bottleneck. Tensorflow should use `assign` function to initialize them.

Predefined loss functions include `tf.losses.mean_squared_error(y_pred, y)`.

### PyTorch

- Tensor: Imperative ndarray, but runs on GPU 
- Variable: Node in a computational graph, stores data and gradient
- Module: A neural network layer, may store state or learnable weights

`Variable` enables autograd, using parameter `requires_grad` to set whether need to record gradients.

`nn` can define model as a sequence of layers, and also defines common loss functions. `y_pred = model(x)` feed data to the model, `loss = loss_fn(y_pred, y)` feed prediction to loss function, then `model.zero_grad()` and `loss.backward()` compute all gradients.

A DataLoadeer wraps a Dataset adn provides minibatching, shuffling, multithreading.

- TensorFlow: build graph once, then run many times (static graphs)
- PyTorch: each forward pss defines a new graph (dynamic graph)

Static graphs can optimize the graph before running, serialize it and tun without accessing to the code. Dynamic graphs are more convenient in conditional situations and loops.

Dynamic graph applications: recurrent networks, recursice networks, modular networks...