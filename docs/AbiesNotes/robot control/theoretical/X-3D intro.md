**这里用来记录短学期报告准备过程中的一些东西。**

!!! warning "注意"

    Generated by ai

## 激活函数介绍

### 把它想象成“开关”和“过滤器”

想象一下，你是一个大脑神经元。你从眼睛、耳朵等地方接收信号（数据）。这些信号有强有弱。

1.  **接收信号（线性计算）**：
    首先，你会把所有这些信号加起来。如果一个信号很重要，你就给它更大的权重；如果不重要，就给它小权重。最后，你还会加上自己的“心情”（偏置项），比如你今天比较乐观还是悲观。
    _这就像是计算总分：`总分 = (信号1 × 权重1) + (信号2 × 权重2) + ... + 心情`_

2.  **决定是否做出反应（激活函数）**：
    现在你有了这个“总分”，但你不会对任何微小的信号都做出反应，对吧？你需要判断这个信号是否**强到了值得你做出反应的程度**。
    - 如果总分超过了某个“阈值”（比如非常响的声音），你就会**兴奋**，并向下一个神经元发送一个强烈的信号。
    - 如果总分很低（比如微弱的杂音），你就**保持平静**，什么都不做。

**这个“判断过程”——决定是否反应以及反应多大——就是激活函数所做的事情。** 它是一个“开关”或“过滤器”，决定了信息能否通过这个神经元继续传递下去。

### 为什么这个“开关”如此重要？

如果没有这个“开关”（激活函数），神经网络就只会做简单的**加减乘除**。无论堆多少层，它最终都只能画出一条**笔直的直线**来分割数据（只能解决线性问题）。

但现实世界的问题复杂得多，比如如何区分猫和狗的图片？它们的区别绝不是一条直线能分清的。我们需要**曲折复杂的曲线**才能区分（非线性问题）。

**激活函数就是这个“ingredient”**：
它给每个神经元的输出引入了**扭曲和弯曲**。当千千万万个这样的神经元组合在一起时，整个网络就能画出极其复杂的曲线，从而学会识别猫脸、理解语言、预测天气等无比复杂的任务。

### 几个常见的“开关”类型（激活函数）

1.  **ReLU（最简单直接的开关）**：

    - **规则**：如果输入分数 > 0，就让分数原样通过；如果分数 <= 0，就直接把它关掉（变为 0）。
    - **像什么**：像一个**整流器**，只允许正电流通过，负电流直接归零。这是目前最常用、最受欢迎的激活函数，因为它效果又好，计算又简单。

2.  **Sigmoid（平滑的开关）**：

    - **规则**：把任何分数都压缩到 0 和 1 之间。
    - **像什么**：像一个**投票器**或**概率转换器**。输入一个很大的正数，输出接近 1（100%同意）；输入一个很大的负数，输出接近 0（0%同意）；输入 0，输出 0.5（一半一半）。它输出非常平滑的结果，常用于输出层做概率预测（比如判断一张图是猫的概率是 90%）。

3.  **Tanh（双曲正切函数）**：
    - **规则**：把任何分数都压缩到 -1 和 1 之间。
    - **像什么**：像一个**更强大的 Sigmoid**。它的输出范围更广（-1 到 1），且平均值是 0，这使得它的优化效果有时比 Sigmoid 更好，常用于隐藏层。

## 卷积核介绍

### 1. 卷积核是什么？

在计算机视觉和深度学习里，**卷积核**就是一个小矩阵（通常大小是 3×3、5×5、7×7 等），它会在图片上“滑动”，并与局部像素做运算。
这个过程叫做 **卷积**。

👉 卷积核可以理解为 **一块特殊的滤镜**，它决定了在处理图像时关注什么特征。

### 2. 卷积核是怎么工作的？

计算步骤：

1. 把卷积核放在图片的一个小区域上（例如 3×3 的区域）。
2. 对应元素相乘再相加，得到一个数。
3. 卷积核滑动到下一个位置，重复计算。

这样就得到了一个新的矩阵（即“特征图”），它表示图片里某种特征的分布。

### 3. 卷积核的作用

不同的卷积核，可以提取不同的特征：

- **边缘检测核**：突出图像中的边缘。
- **模糊核**：让图片变得模糊。
- **锐化核**：让图片变得清晰。
- **在深度学习中**：卷积核的数值不是人为设定的，而是 **通过训练学习得到的**，可以自动提取识别任务需要的特征（比如边缘、纹理、形状、甚至人脸）。

#### (1) 边缘检测核

例子：

$$
K =
\begin{bmatrix}
-1 & -1 & -1 \\
0 & 0 & 0 \\
1 & 1 & 1
\end{bmatrix}
$$

- 解释：它在竖直方向上计算差异。
- 结果：边缘区域差异大 → 输出值大，突出边缘；平坦区域差异小 → 输出接近 0。
- 所以它能“抓住边缘”。

#### (2) 模糊核

例子：

$$
K = \frac{1}{9}
\begin{bmatrix}
1 & 1 & 1 \\
1 & 1 & 1 \\
1 & 1 & 1
\end{bmatrix}
$$

- 解释：对 3×3 区域的像素做平均。
- 结果：细节（高频信息）被平滑掉，看起来更模糊。

#### (3) 锐化核

例子：

$$
K =
\begin{bmatrix}
0 & -1 & 0 \\
-1 & 5 & -1 \\
0 & -1 & 0
\end{bmatrix}
$$

- 解释：中央像素被放大（权重大于 1），周围像素被减弱（负权重）。
- 结果：边缘对比度被增强，图像看起来更清晰。

### 4. 总结

- **卷积核的作用取决于里面的数字（权重）分布**。
- 不同的数字组合，决定了它是提取边缘、模糊、锐化，还是其他特征。
- 在深度学习中，这些“数字组合”不是人工设置的，而是 **网络自动学习出来的最优配方**，用来完成任务（如识别猫、狗、人脸）。

## 动态核介绍

### 1. 动态核的基本定义

在传统的 CNN 里，**卷积核（Convolution Kernel）** 的参数是固定的，一旦训练完成，它就不会再随输入变化。
而 **动态核（Dynamic Kernel）** 则是指：
👉 **卷积核的权重不再固定，而是根据输入样本动态生成或调整。**

这样，模型在处理不同的输入时，能自适应地使用不同的特征提取方式。

### 2. 为什么需要动态核

- **固定卷积核的局限**：一组固定参数对所有输入都一样，可能无法捕捉到不同场景中的差异。
- **动态核的优势**：让网络像“变焦镜头”一样，根据输入的内容调整自己，增强了模型的灵活性和适应性。

举个比喻：

- **固定卷积核**：像戴一副固定度数的眼镜，看任何东西都一样。
- **动态卷积核**：像戴一副智能眼镜，可以根据远近、光线自动调节焦距和亮度。

### 3. 动态核的实现方式

常见的实现方式有几种：

1. **条件卷积（CondConv, 2019 Google 提出）**

   - 为每个卷积层准备多组核，
   - 然后用一个小网络（条件网络）根据输入决定每组核的加权组合。

2. **动态卷积（Dynamic Convolution, 2020 Facebook 提出）**

   - 类似 CondConv，但更高效，适合 NLP 和 CV。
   - 在 NLP 中，它甚至被拿来替代部分注意力机制（attention）。

3. **核生成网络（Kernel Generation Network）**

   - 通过一个额外的子网络，直接根据输入生成新的卷积核权重。

### 4. 应用场景

- **图像识别**：提高在复杂、多样环境下的鲁棒性。
- **视频理解**：根据时间序列变化动态调整卷积核。
- **自然语言处理（NLP）**：动态卷积被用于提升 Transformer 的效率（比如 Facebook 的 DynamicConv）。
- **轻量化模型**：动态核能在不大幅增加参数量的情况下增强表现力。

### 5. 总结

- **传统卷积核**：固定的，所有输入用同一套滤镜。
- **动态卷积核**：可变的，根据输入内容自适应调整滤镜。
- **意义**：增强模型的灵活性和表达能力，能更好地处理复杂、多样化的数据。

## 神经网络介绍

### 1. 直观理解

神经网络的灵感来自人脑。

- **人脑**：有很多神经元，接收信号 → 处理 → 决定是否激活 → 把信号传给其他神经元。
- **人工神经网络**：用数学函数去模仿这种机制，把输入（数据）逐层传递、加工，最后得到输出（预测结果）。

👉 可以把它想象成一群“信息加工工人”：

- 每个工人（神经元）只做一点点加权计算和判断。
- 但成千上万个工人（神经元）一起合作，就能完成复杂任务（比如识别猫狗、翻译语言）。

### 2. 数学上的结构

一个神经网络由 **层（Layers）** 组成：

- **输入层（Input Layer）**：接收数据（如一张图的像素、一个句子的词向量）。
- **隐藏层（Hidden Layers）**：多层神经元处理并提取特征。
- **输出层（Output Layer）**：给出结果（比如“这是猫”的概率 90%）。

每个神经元的计算过程：

$$
输出 = 激活函数( \sum (输入 \times 权重) + 偏置 )
$$

其中：

- **权重 (Weight)**：表示输入的重要性。
- **偏置 (Bias)**：表示整体的调整。
- **激活函数**：决定神经元是否激活，赋予网络非线性能力。

### 3. 神经网络的学习过程

神经网络需要“训练”：

1. **正向传播（Forward Propagation）**
   输入数据经过网络计算，得到预测结果。
2. **计算误差（Loss Function）**
   比较预测和真实答案之间的差距。
3. **反向传播（Backpropagation）**
   误差往回传，调整权重和偏置。
4. **梯度下降（Gradient Descent）**
   通过不断迭代，让网络的预测越来越接近真实。

👉 就像学生做题 → 检查错题 → 老师批改 → 下次改进。

### 4. 神经网络能做什么？

- 图像识别（识别猫、狗、人脸）
- 自然语言处理（翻译、聊天机器人）
- 游戏 AI（AlphaGo）
- 语音识别、自动驾驶、医学诊断……

## 多层感知机 MLP 介绍

### 1. 什么是感知机（Perceptron）

- 感知机是最简单的神经元模型。
- 它的计算公式：

$$
y = f\left(\sum (w_i \cdot x_i) + b \right)
$$

其中：

- $x_i$ = 输入
- $w_i$ = 权重
- $b$ = 偏置
- $f$ = 激活函数

👉 可以理解为：把输入加权求和，再通过一个“开关”（激活函数），得到输出。

### 2. 多层感知机（MLP）的结构

单个感知机很弱，只能解决线性问题。
所以人们把很多感知机堆叠起来，形成 **多层感知机（MLP）**。

MLP 一般包括：

- **输入层（Input Layer）**：接收数据（如图像像素、文本向量）。
- **隐藏层（Hidden Layers）**：由多个感知机组成，每层输出作为下一层的输入。
- **输出层（Output Layer）**：给出最终结果（分类、回归等）。

👉 关键点：**隐藏层越多、神经元越多，MLP 的表达能力越强。**

### 3. MLP 的计算流程

1. 输入层接收数据（如 28×28 像素的手写数字 → 展平成 784 维向量）。
2. 数据逐层传递，每层神经元做：加权求和 → 激活函数。
3. 输出层得到预测结果。

### 4. 激活函数的重要性

- 如果没有激活函数，MLP 只能表示“直线”。
- 有了激活函数，MLP 就能表示复杂的“曲线关系”，解决非线性问题（比如区分猫和狗）。

### 5. MLP 的特点

✅ 优点：

- 结构简单，易于理解和实现。
- 是深度学习的基础模型，适合入门。
- 可以处理任意形式的输入（图像、文本、表格数据等）。

⚠️ 缺点：

- 参数量可能很大（输入维度 × 神经元数）。
- 没有利用数据的空间或序列结构（图像要用 CNN，序列要用 RNN/Transformer）。

### 6. 总结

- **MLP = 多层感知机 = 最经典的前馈神经网络**。
- 由输入层、多个隐藏层和输出层组成。
- 每个神经元：加权求和 → 激活函数 → 输出。
- 通过训练（反向传播 + 梯度下降）自动调整参数。
- 是深度学习最基础的模型，但在图像/语言等任务中通常被更专业的网络（CNN、RNN、Transformer）替代。

## CNN 介绍

### 1. 什么是 CNN？

CNN （卷积神经网络, Convolutional Neural Network）是一种 **深度学习模型**，专门擅长处理 **具有空间结构的数据**，比如图像、视频、语音和时间序列。

与普通的全连接神经网络（MLP）相比，CNN 通过 **卷积运算** 提取局部特征，可以自动从原始数据中学到边缘、纹理、形状等信息。

👉 **一句话理解**：
CNN 就像一个“特征提取工厂”，它能自动从图片里找到猫的耳朵、狗的鼻子、汽车的轮子等关键信息，而不用人工设计特征。

---

### 2. CNN 的核心结构

CNN 主要由三类层组成：

1. **卷积层（Convolutional Layer）**

   - 用卷积核（小矩阵）在输入数据上滑动，提取局部特征。
   - 低层卷积核：检测边缘、颜色块。
   - 高层卷积核：检测复杂形状（比如眼睛、脸）。

2. **池化层（Pooling Layer）**

   - 对特征图进行缩小（比如取最大值 MaxPooling 或平均值 AveragePooling）。
   - 作用：减少计算量、降低过拟合、保留重要信息。

3. **全连接层（Fully Connected Layer）**

   - 在最后阶段，把卷积层提取的特征组合起来，用于分类或回归。
   - 例如：输出“这是一只猫”的概率 = 0.92。

---

### 3. CNN 的工作流程（图像分类为例）

1. **输入**：一张图片（比如猫）。
2. **卷积**：卷积核提取边缘、纹理等局部特征。
3. **池化**：缩小特征图，减少冗余。
4. **多层卷积 + 池化**：逐层提取越来越复杂的特征（耳朵 → 脸 → 整只猫）。
5. **全连接层**：综合所有特征，进行分类。
6. **输出**：预测结果，比如「猫：92%，狗：7%，其他：1%」。

---

### 4. CNN 的优势

- **局部连接**：每个卷积核只关注局部区域，适合捕捉图像局部模式。
- **参数共享**：同一个卷积核在全图使用，减少参数数量，提升效率。
- **层次结构**：底层学简单特征，高层学复杂特征。
- **泛化能力强**：能自动提取特征，不需要手工设计。

---

### 5. 现实应用

- **图像识别**（猫狗识别、人脸识别）
- **目标检测**（自动驾驶中的行人检测）
- **医学影像**（肿瘤识别、X 光分析）
- **语音识别**（声谱图特征提取）
- **自然语言处理**（文本分类、情感分析）

## RNN 介绍

### 1. 什么是 RNN？

RNN （循环神经网络，Recurrent Neural Network）是一种**专门处理序列数据的神经网络**，擅长处理时间或顺序相关的信息，例如：

- 文字序列 → 句子生成、翻译
- 语音信号 → 语音识别
- 视频帧 → 动作识别
- 时间序列数据 → 股票预测、传感器数据分析

RNN 的核心思想是 **记住过去的信息**，把序列前面输入的状态影响后面的输出。

---

### 2. RNN 的结构特点

普通神经网络（如 MLP、CNN）假设输入独立，而 RNN 引入了 **循环连接**：

- 每个时间步的输出不仅依赖当前输入，还依赖上一个时间步的隐藏状态（hidden state）。
- 公式表示：

$$
h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b_h)
$$

$$
y_t = g(W_{hy} h_t + b_y)
$$

其中：

- $x_t$：当前时间步输入
- $h_t$：当前隐藏状态（记忆）
- $h_{t-1}$：上一个时间步的隐藏状态
- $y_t$：当前输出
- $f, g$：激活函数

> 隐藏状态 $h_t$ 就像“记忆”，能够让网络记住序列信息。

---

### 3. RNN 的工作流程（举例：文字生成）

1. 输入序列：**“今天天气很好”**
2. 第一个字 “今” 输入 → 计算隐藏状态 h1 → 输出预测
3. 第二个字 “天” 输入 → 隐藏状态 h1 + 当前输入 → 更新 h2 → 输出预测
4. 第三个字 “天” 输入 → 隐藏状态 h2 + 当前输入 → 更新 h3 → 输出预测
5. …
6. 最终输出：模型可以根据前面的文字预测下一个字，生成连贯句子

---

### 4. RNN 的优势

- 能处理 **序列数据**，理解上下文信息
- 可以 **共享参数**，不同时间步使用同一组权重，减少参数量
- 可应用于文本、语音、视频、金融等多种时序任务

---

### 5. RNN 的问题

- **梯度消失/爆炸**：长序列训练困难
- **短期记忆能力有限**：对于长距离依赖（例如文章开头的信息影响结尾）难以记住

> 解决方案：出现了 **LSTM（长短期记忆网络）** 和 **GRU（门控循环单元）**，专门解决长序列记忆问题。


